{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101ad645-d469-48c5-bcc0-7718ab393392",
   "metadata": {},
   "source": [
    "# Creating Prediction Models: Random Forest, XGBoost, Neural Networks. With Time-Sensitive Cross-Validation and Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3df790-cd49-4b54-b0d4-db217a4bcda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we import the libraries and classes we'll use\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import datetime\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86f422-93c4-48c2-83e1-2f9784415c35",
   "metadata": {},
   "source": [
    "## A Heads-Up on the games DataFrame\n",
    "\n",
    "Let's explain the relevant columns in the games DataFrame:\n",
    "- 2 team name columns: 'team_name_home', 'team_name_away'. Note that at some point in time, the Charlotte Bobcats changed their name to the Charlotte Hornets and the New Orleans Hornets became the New Orleans Pelicans. I rename them for consistency. These will then be OneHotEncoded.\n",
    "- 1 date column: 'game_date'. Since many models can't process timeframes, we set the earliest date in the DF to 0 and the rest are transformed into the number of days since the earliest date.\n",
    "- 6 win-loss % columns: 'record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses', 'home_wl%', 'away_wl%'.\n",
    "- 2 Elo columns: 'elo_home', 'elo_away'. These are Elos *before* the game. For more details on these ratings, see Step 3 of the \"Data Collection for RF, XGB, NN\" notebook or the ReadMe in https://github.com/Matija-Sreckovic/NBA-Simple-Prediction-Models.\n",
    "- 2 team OffRtg-DefRtg-GmSc ratings columns: 'rating_home', 'rating_away'. For more details on these scores, see Step 5 of \"Data Collection for RF, XGB, NN\" notebook or the ReadMe in https://github.com/Matija-Sreckovic/NBA-Prediciton-Model.\n",
    "- 12 individual OffRtg-DefRtg-GmSc ratings columns: 'rating_home_player1', ..., 'rating_home_player5', 'rating_away_player1', ..., 'rating_away_player5'. 5 for each of the teams' best players' ratings.\n",
    "- 8 Offensive/Defensive Rating columns: 'ortg_home', 'ortg_away', 'drtg_home', 'drtg_away', 'home_ortg_last_season', ... Contains teams' offensive/defensive ratings up to that point in the season, or for all of last season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7e97cc-2110-40a5-845a-0f6bed7b634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_games_dataframe(games, columns_to_drop):\n",
    "    \"\"\"Here, we just load the games DataFrame and create a bunch of useful columns (difference between Elos, \"Offrtg-Defrtg-GmSc\" ratings, home/away net \n",
    "    ratings, difference between net ratings, WL% difference, ...\n",
    "    The columns_to_drop argument should take values 0, 1 or 2: if it's 0, we don't remove any columns. \n",
    "    If it's 1, we remove a lot of them: individual Elos, individual team \"ODG\" ratings, individual player ODG ratings, individual offensive/defensive \n",
    "    ratings for both this season and last, individual WL%, and individual net ratings last season. It turns out the leaving the individual net ratings\n",
    "    and player ODG ratings in helps the model perform better.\n",
    "    If it's 2, we remove all individual columns and keep only the differences between the teams' stats.\n",
    "    \"\"\"\n",
    "    #drop the obsolete 'matchup_home' column, and game_id since it's just an index column\n",
    "    games = games.drop(columns = ['matchup_home'])\n",
    "    games = games.drop(columns = ['game_id'])\n",
    "\n",
    "    #turn the game_date into datetime format\n",
    "    games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "    \n",
    "    #drop a column to prevent data leakage - this is the margin of victory of the home team (negative if the home team lost)\n",
    "    games = games.drop(columns = ['plus_minus_home'])\n",
    "    \n",
    "    #add the difference in Elo ratings as a column\n",
    "    games['elo_diff'] = games['elo_home'] - games['elo_away']\n",
    "    #add the gamescore-offrtg-defrtg rating difference as a column\n",
    "    games['rating_diff'] = games['rating_home'] - games['rating_away']\n",
    "    #add the rating *difference* between the nth best players, but don't drop the individual ratings\n",
    "    for i in list(range(1,6)):\n",
    "        games['rating_player_diff' + str(i)] = games['rating_home_player' + str(i)] - games['rating_away_player' + str(i)]\n",
    "    #add home and away net rating as difference of offrtg and defrtg\n",
    "    games['home_netrtg'] = games['ortg_home'] - games['drtg_home']\n",
    "    games['away_netrtg'] = games['ortg_away'] - games['drtg_away']\n",
    "    #add home net rating and away net rating from last season\n",
    "    games['home_netrtg_last_season'] = games['home_ortg_last_season'] - games['home_drtg_last_season']\n",
    "    games['away_netrtg_last_season'] = games['away_ortg_last_season'] - games['away_drtg_last_season']\n",
    "    #add difference in win-loss percentage\n",
    "    games['wl%_diff'] = games['home_wl%'] - games['away_wl%']\n",
    "    #add net rating difference\n",
    "    games['netrg_diff'] = games['home_netrtg'] - games['away_netrtg']\n",
    "    #add netrtg difference from last season\n",
    "    games['netrtg_diff_last_season'] = games['home_netrtg_last_season'] - games['away_netrtg_last_season']\n",
    "\n",
    "    #add games played home/away (games played so far this season)\n",
    "    games['GP_home'] = games['record_home_wins'] + games['record_home_losses']\n",
    "    games['GP_away'] = games['record_away_wins'] + games['record_away_losses']\n",
    "\n",
    "    #in 2013-14, the Charlotte Hornets were called the Bobcats, and (maybe) the New Orleans Pelicans were called the Hornets\n",
    "    games.replace('Charlotte Bobcats', 'Charlotte Hornets', inplace=True)\n",
    "    games.replace('New Orleans Hornets', 'New Orleans Pelicans', inplace=True)\n",
    "\n",
    "    if columns_to_drop not in [0,1,2]:\n",
    "        raise ValueError(\"Wrong value of columns_to_drop: please enter 0, 1 or 2.\")\n",
    "\n",
    "    if (columns_to_drop == 1) or (columns_to_drop == 2):\n",
    "        games = games.drop(columns = ['elo_home', 'elo_away'])\n",
    "        games = games.drop(columns = ['rating_home', 'rating_away'])\n",
    "        games = games.drop(columns = ['ortg_home', 'drtg_home'])\n",
    "        games = games.drop(columns = ['ortg_away', 'drtg_away'])\n",
    "        games = games.drop(columns = ['home_ortg_last_season', 'home_drtg_last_season', 'away_ortg_last_season', 'away_drtg_last_season'])\n",
    "        games = games.drop(columns = ['record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses'])\n",
    "        games = games.drop(columns = ['home_wl%', 'away_wl%'])\n",
    "        games = games.drop(columns = ['home_netrtg_last_season', 'away_netrtg_last_season'])\n",
    "\n",
    "    if columns_to_drop == 2:\n",
    "        games = games.drop(columns = ['home_netrtg', 'away_netrtg'])\n",
    "        games = games.drop(columns = ['rating_home_player' + str(i), 'rating_away_player' + str(i)])\n",
    "\n",
    "    #turning dates into days since first game in table\n",
    "    games['game_date'] = (games['game_date'] - games['game_date'].min()) / pd.Timedelta(days=1)\n",
    "\n",
    "    #get target column\n",
    "    wl_home = games['wl_home']\n",
    "    games = games.drop(columns = 'wl_home')\n",
    "    \n",
    "    return games, wl_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2734890-bf29-4c82-80a1-5f7b7ce1b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessor(games, classifier, use_pca):\n",
    "    \"\"\"\n",
    "    The preprocessing step. The classifier argument takes values \"RF\", \"XGB\" or \"NN\". \n",
    "    The PCA argument should be 1 (use PCA) or 0 (don't use PCA).\n",
    "    If we use PCA after transforming the columns, we need to rescale the numerical features.\n",
    "    NB: the target column needs to be removed from the games table before applying the preprocessor!\n",
    "    \"\"\"\n",
    "    categorical_columns = ['team_name_home', 'team_name_away', 'season_type']\n",
    "    numerical_columns = [col for col in games.columns if col not in categorical_columns]\n",
    "    if classifier not in [\"RF\", \"XGB\", \"NN\"]:\n",
    "        raise ValueError(\"The classifier argument should take values in [\\\"RF\\\", \\\"XGB\\\", \\\"NN\\\"]\")\n",
    "    if use_pca:\n",
    "        transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('team_names', OneHotEncoder(), categorical_columns),  # Apply OneHotEncoder to team columns\n",
    "                ('standard_scale_num_cols', StandardScaler(), numerical_columns) # Scale numerical columns for PCA\n",
    "            ],\n",
    "        )\n",
    "        pca = PCA(n_components=30)  # Reduce to 30 dimensions\n",
    "        return make_pipeline(transformer, pca)\n",
    "    elif classifier in [\"RF\", \"XGB\"]:\n",
    "        if not use_pca:\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('team_names', OneHotEncoder(), categorical_columns)  # Apply OneHotEncoder to team columns\n",
    "                ],\n",
    "                remainder='passthrough'  # Keep other columns as is (e.g., numeric columns)\n",
    "            )\n",
    "            return preprocessor    \n",
    "    else:\n",
    "        numerical_columns = [col for col in games.columns if col not in categorical_columns]\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('team_names', OneHotEncoder(), categorical_columns),  # Apply OneHotEncoder to team columns\n",
    "                ('numerical_for_rescaling', StandardScaler(), numerical_columns),\n",
    "            ],\n",
    "        )\n",
    "        return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3109be-2e5b-456d-a411-fa80e3c692f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign seasons\n",
    "def assign_season(game_date):\n",
    "    year = game_date.year\n",
    "    if game_date.month < 10:  # If before October, it's part of the previous season\n",
    "        year -= 1\n",
    "    return f\"{year}-{year + 1}\"\n",
    "\n",
    "# Add season column\n",
    "games['season'] = games['game_date'].apply(assign_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ac966d-61a9-47c1-a148-d285981ffbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_val_indices(games, n_folds=3):\n",
    "    \"\"\"\n",
    "    We need time-dependent cross-validation, meaning that no row of the test set can go before the train set. \n",
    "    However, I also want the train and test sets to consist of entire *seasons*; with TimeSeriesSplit they end at a random point in the season.\n",
    "    I want to train on at least 5 seasons, hence current_training_number = 4 and n_folds <= 7 (recall that there are 12 seasons, from\n",
    "    2013-14 to 2024-25).\n",
    "    I want the training_set:test_set ratio to be at least 4:1, hence the line new_testing_number <= new_number // 4.\n",
    "    So I create n_folds train-test pairs, where each new training set is a bit bigger than the last (at least 5 seasons, but less than 12).\n",
    "    The test set consists of seasons immediately following the train set.\n",
    "    \"\"\"\n",
    "    if n_folds > 7:\n",
    "        raise ValueError(\"Too many folds!\")\n",
    "    training_numbers = []\n",
    "    training_indices = [] # an element of this list is a list of indices\n",
    "    testing_numbers = []\n",
    "    testing_indices = [] # an element of this list is a list of indices\n",
    "    n_folds_left = n_folds\n",
    "    current_training_number = 4\n",
    "    for i in list(range(0, n_folds)):\n",
    "        new_number = random.randint(current_training_number + 1 , 12 - n_folds_left)\n",
    "        training_numbers.append(new_number)\n",
    "        new_testing_number = random.randint(1, new_number // 4)\n",
    "        testing_numbers.append(new_testing_number) # these indicate on how *many* seasons we test\n",
    "        training_indices_to_append = []\n",
    "        testing_indices_to_append = []\n",
    "        for j in list(range(2013, 2013 + new_number)):\n",
    "            season_rows = games[games['season'] == str(j) + \"-\" + str(j+1)]\n",
    "            season_indices = season_rows.index.tolist()\n",
    "            for idx in season_indices:\n",
    "                training_indices_to_append.append(idx)\n",
    "        training_indices.append(training_indices_to_append)\n",
    "        for j in list(range(2013 + new_number, 2013 + new_number + testing_numbers[i])):\n",
    "            season_rows = games[games['season'] == str(j) + \"-\" + str(j+1)]\n",
    "            season_indices = season_rows.index.tolist()\n",
    "            for idx in season_indices:\n",
    "                testing_indices_to_append.append(idx)\n",
    "        testing_indices.append(testing_indices_to_append)\n",
    "        n_folds_left -= 1\n",
    "        current_training_number = new_number\n",
    "    return training_indices, testing_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88406ae-9b88-4af7-9afb-d98b890b6c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'game_id', 'game_date', 'team_name_home',\n",
       "       'team_name_away', 'matchup_home', 'wl_home', 'record_home_wins',\n",
       "       'record_home_losses', 'record_away_wins', 'record_away_losses',\n",
       "       'elo_home', 'elo_away', 'plus_minus_home', 'season_type', 'home_wl%',\n",
       "       'away_wl%', 'rating_home', 'rating_away', 'rating_home_player1',\n",
       "       'rating_away_player1', 'rating_home_player2', 'rating_away_player2',\n",
       "       'rating_home_player3', 'rating_away_player3', 'rating_home_player4',\n",
       "       'rating_away_player4', 'rating_home_player5', 'rating_away_player5',\n",
       "       'ortg_home', 'drtg_home', 'ortg_away', 'drtg_away',\n",
       "       'home_ortg_last_season', 'home_drtg_last_season',\n",
       "       'away_ortg_last_season', 'away_drtg_last_season', 'season'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee34916-7aaa-45b3-8450-6146739ec892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (3105312796.py, line 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 100\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(str(best_keys_per_fold_and_setting[i]) + \": \" + str(best_scores_per_fold_and_setting[i])))\u001b[0m\n\u001b[1;37m                                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "#Random Forest hyperparameter tuning:\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "\n",
    "# Add season column\n",
    "games['season'] = games['game_date'].apply(assign_season)\n",
    "\n",
    "#Get the training and testing indices for cross-validation\n",
    "training_indices, testing_indices = get_cross_val_indices(games)\n",
    "\n",
    "#each element of these two lists is itself a 3-element list, containing the best scores for each of the 3 train-test splits\n",
    "max_keys = []\n",
    "max_values = []\n",
    "best_keys_per_fold_and_setting = [] \n",
    "best_scores_per_fold_and_setting = []\n",
    "for i in [True, False]:\n",
    "    for j in list(range(0,3)):\n",
    "        max_key_folds = [0,0,0]\n",
    "        max_value_folds = [0,0,0]\n",
    "        \n",
    "        games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "        games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "    \n",
    "        # Add season column\n",
    "        games['season'] = games['game_date'].apply(assign_season)\n",
    "    \n",
    "        # Load the games DataFrame - drop relevant columns\n",
    "        games, wl_home = load_games_dataframe(games, j)\n",
    "    \n",
    "        #Get the preprocessor - the i parameter decides if we use PCA or not.\n",
    "        preprocessor = get_preprocessor(games, \"XGB\", i)\n",
    "        \n",
    "        #Candidate parameter values\n",
    "        n_estimators_list = [50, 100, 400]\n",
    "        min_samples_split_list = [2, 10, 100]\n",
    "        min_samples_leaf_list = [1, 10, 100]\n",
    "        max_features_list = ['sqrt', None]\n",
    "        max_leaf_nodes_list = [100, 1000, None]\n",
    "        \n",
    "        #Cartesian product of the lists\n",
    "        all_combinations = list(product(\n",
    "            n_estimators_list,\n",
    "            min_samples_split_list,\n",
    "            min_samples_leaf_list,\n",
    "            max_features_list,\n",
    "            max_leaf_nodes_list\n",
    "        ))\n",
    "                \n",
    "        # Create a copy of games since we're going to be changing the seasons column, and we don't want it affected for the other models.\n",
    "        games_rf = games.copy()\n",
    "        \n",
    "        # Create a mapping dictionary\n",
    "        seasons = [f\"{year}-{year+1}\" for year in range(2013, 2025)]\n",
    "        season_mapping = {season: k for k, season in enumerate(seasons)}\n",
    "        \n",
    "        # Map the 'season' column to numeric values\n",
    "        games_rf['season'] = games_rf['season'].map(season_mapping)\n",
    "        \n",
    "        #Accuracy dict\n",
    "        accuracies = {combination : [] for combination in all_combinations}\n",
    "        avg_accuracies = {combination : 0 for combination in all_combinations}\n",
    "        \n",
    "        #Access individual elements of a combination and fit and train the model. Keep all of the scores, but especially the best score for each\n",
    "        #combination and each fold of the cross-validation.\n",
    "        for combination in all_combinations:\n",
    "            clear_output(wait=True)\n",
    "            print(combination)\n",
    "            n_estimators = combination[0]       # First element\n",
    "            min_samples_split = combination[1]  # Second element\n",
    "            min_samples_leaf = combination[2]   # Third element\n",
    "            max_features = combination[3]       # Fourth element\n",
    "            max_leaf_nodes = combination[4]     # Fifth element\n",
    "            classifier = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                               max_features = max_features, max_leaf_nodes=max_leaf_nodes, n_jobs=8)\n",
    "            model = make_pipeline(preprocessor, classifier)\n",
    "            for k in list(range(0,3)):\n",
    "                data_train = games_rf.loc[training_indices[k]]\n",
    "                target_train = wl_home.loc[training_indices[k]]\n",
    "                data_test = games_rf.loc[testing_indices[k]]\n",
    "                target_test = wl_home.loc[testing_indices[k]]\n",
    "                model.fit(data_train, target_train)\n",
    "                score = model.score(data_test, target_test)\n",
    "                accuracies[combination].append(score)\n",
    "                if score > max_value_folds[k]:\n",
    "                    max_value_folds[k] = score\n",
    "                    max_key_folds[k] = combination\n",
    "            avg_accuracies[combination] = statistics.mean(accuracies[combination])\n",
    "        accuracies_series = pd.Series(accuracies)\n",
    "        accuracies_series.to_csv(base_dir / (\"tuning_rf_accuracies_\" + str(j) + \"-\" + str(i) + \".csv\"))\n",
    "        avg_accuracies_series = pd.Series(avg_accuracies)\n",
    "        avg_accuracies_series.to_csv(base_dir / (\"tuning_rf_avg_accuracies_\" + str(j) + \"-\" + str(i) + \".csv\"))\n",
    "        max_key = max(avg_accuracies, key=avg_accuracies.get)\n",
    "        max_keys.append(max_key)\n",
    "        max_values.append(avg_accuracies[max_key])\n",
    "        best_keys_per_fold_and_setting.append(max_key_folds)\n",
    "        best_scores_per_fold_and_setting.append(max_value_folds)\n",
    "\n",
    "for i in range(0,6):\n",
    "    print(str(max_keys[i]) + \": \" + str(max_values[i]))\n",
    "    print(str(best_keys_per_fold_and_setting[i]) + \": \" + str(best_scores_per_fold_and_setting[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82bbabc8-2d26-46a7-9bd9-c7fd406dac85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11502, 11503, 12186, 12188, 14135, 14136)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(training_indices[0]), min(testing_indices[0]), max(training_indices[1]), min(testing_indices[1]), max(training_indices[2]), min(testing_indices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80aaa553-5ece-4e2c-a29c-d7620a70e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50, 100, 10, 'sqrt', 1000.0], [50, 100, 1, nan, 100.0], [50, 2, 10, nan, nan]]: [0.6536270413976453, 0.6709601873536299, 0.6940874035989717]\n",
      "[[100, 100, 100, nan, nan], [100, 10, 10, nan, 100.0], [400, 2, 1, 'sqrt', 1000.0]]: [0.6521078617546525, 0.6703747072599532, 0.6889460154241646]\n",
      "[[50, 10, 100, 'sqrt', 100.0], [50, 100, 10, 'sqrt', 1000.0], [50, 2, 1, nan, 1000.0]]: [0.6502088872009115, 0.6692037470725996, 0.6915167095115681]\n",
      "[[50, 2, 1, 'sqrt', 100.0], [50, 2, 10, 'sqrt', 1000.0], [50, 2, 10, 'sqrt', 1000.0]]: [0.6585643752373718, 0.6692037470725996, 0.6812339331619537]\n",
      "[[100, 2, 100, 'sqrt', 1000.0], [50, 2, 100, nan, nan], [50, 100, 1, nan, 100.0]]: [0.6578047854158754, 0.6703747072599532, 0.6760925449871465]\n",
      "[[50, 100, 100, 'sqrt', nan], [50, 100, 100, nan, 1000.0], [50, 100, 10, nan, 1000.0]]: [0.6574249905051273, 0.6686182669789227, 0.6812339331619537]\n"
     ]
    }
   ],
   "source": [
    "#The program crashed multiple times, so all I'm left with are the csv files. Here I'm reconstructing the best_keys/scores_per_fold_and_setting\n",
    "\n",
    "best_keys_per_fold_and_setting = [] \n",
    "best_scores_per_fold_and_setting = []\n",
    "\n",
    "for i in [True, False]:\n",
    "    for j in list(range(3)):\n",
    "        df = pd.read_csv(base_dir / (\"tuning_rf_accuracies_\" + str(j) + \"-\" + str(i) + \".csv\"))\n",
    "        df.columns = ['n_estimators', 'min_samples_split', 'min_samples_leaf', 'max_features', 'max_leaf_nodes', 'scores']\n",
    "        # Split the single column into three columns\n",
    "        df_split = df['scores'].str.strip(\"[]\").str.split(\",\", expand=True)\n",
    "\n",
    "        # Convert each column to float\n",
    "        df_split = df_split.astype(float)\n",
    "\n",
    "        # Assign new column names if desired\n",
    "        df_split.columns = [\"score1\", \"score2\", \"score3\"]\n",
    "\n",
    "        # Add the new columns back to the original DataFrame\n",
    "        df = pd.concat([df, df_split], axis=1)\n",
    "\n",
    "        max_key_folds = [0,0,0]\n",
    "        max_value_folds = [0,0,0]\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            for k in list(range(3)):\n",
    "                if row['score' + str(k+1)] > max_value_folds[k]:\n",
    "                    max_key_folds[k] = [row['n_estimators'], row['min_samples_split'], row['min_samples_leaf'], row['max_features'], row['max_leaf_nodes']]\n",
    "                    max_value_folds[k] = row['score' + str(k+1)]\n",
    "\n",
    "        best_keys_per_fold_and_setting.append(max_key_folds)\n",
    "        best_scores_per_fold_and_setting.append(max_value_folds)\n",
    "\n",
    "for k in list(range(6)):\n",
    "    print(str(best_keys_per_fold_and_setting[k]) + \": \" + str(best_scores_per_fold_and_setting[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a331d-58e7-4471-8576-6b535b662595",
   "metadata": {},
   "source": [
    "## Conclusion for the Random Forest Model\n",
    "\n",
    "The best performing random forest model is *without* PCA, with *no* \"individual\" columns removed (and differences between the teams' scores added). The best scoring parameters are n_estimators = 50, min_samples_split = 2, min_samples_leaf = 10, max_features = 'sqrt', max_leaf_nodes = 1000. Average accuracy of that model on the randomly selected cross-validation split (3-fold): 66.7%.  \n",
    "\n",
    "- First train-test split: train from 2013-14 to 2021-22 season, test on 2022-23 and 2023-24 seasons. Best result on this split: no PCA, no column dropping, n_estimators = 50, min_samples_split = 2, min_samples_leaf = 10, max_features = None, max_leaf_nodes = None. Accuracy: 65.9%\n",
    "- Second train-test split: train from 2013-14 to 2022-23 season, test on 2023-24 and 2024-25 up to Dec. 16. Best result on this split: with PCA, no column dropping, n_estimators = 50, min_samples_split = 100, min_samples_leaf = 1, max_features = None, max_leaf_nodes = 100. Accuracy: 67.1%\n",
    "- Third train-test split: train from 2013-14 to 2023-24 season, test on 2024-25 up to Dec. 16. Best result on this split: with PCA, no column dropping, n_estimators = 50, min_samples_split = 100, min_samples_leaf = 1, max_features = None, max_leaf_nodes = 100. Accuracy: 69.4%. Larger accuracy here probably due to very small testing set.\n",
    "\n",
    "The conclusion is that reducing the number of components from ~90 to 30 probably **neither increases nor decreases the model's performance.** I was wrong about dropping columns; **the best results are obtained with the maximal number of columns.**\n",
    "Now we move on to the XGBClassifier! We use the same train-test splits in order to compare it to RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89bc16a-a624-4d80-a441-348fe5ee2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = [0,0,0]\n",
    "testing_indices = [0,0,0]\n",
    "training_indices[0] = list(range(11503))\n",
    "testing_indices[0] = list(range(11503, 14136))\n",
    "training_indices[1] = list(range(12187))\n",
    "testing_indices[1] = list(range(12188, 14525))\n",
    "training_indices[2] = list(range(14136))\n",
    "testing_indices[2] = list(range(14136, 14525))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b81f004-c799-4a50-b57d-c746e677931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 5, 10, 20, 1, 400, 1.0)\n",
      "(0.01, 5, 10, 0, 1, 200, 0.7): 0.663298278477765\n",
      "[(0.01, 0, 3, 5, 0.5, 100, 1.0), (0.01, 1, 6, 20, 1, 200, 1.0), (0.2, 0, 6, 0, 1, 100, 0.7)]: [0.6536270413976453, 0.6611039794608472, 0.699228791773779]\n",
      "(0.1, 1, 6, 20, 0.5, 100, 1.0): 0.6605877338954586\n",
      "[(0.01, 5, 10, 0, 1, 400, 1.0), (0.01, 0, 10, 20, 0.5, 200, 0.7), (0.1, 1, 6, 20, 0.5, 100, 1.0)]: [0.6559058108621344, 0.660676080445015, 0.7069408740359897]\n",
      "(0.01, 1, 10, 5, 0.5, 200, 1.0): 0.6645830755227319\n",
      "[(0.01, 5, 6, 0, 1, 200, 1.0), (0.01, 0, 6, 0, 0.5, 100, 1.0), (0.1, 5, 10, 0, 0.5, 400, 1.0)]: [0.6536270413976453, 0.660676080445015, 0.6966580976863753]\n",
      "(0.01, 0, 6, 0, 0.5, 400, 1.0): 0.6637200981937426\n",
      "[(0.2, 5, 3, 0, 1, 100, 1.0), (0.01, 0, 3, 20, 1, 400, 1.0), (0.2, 0, 6, 0, 0.5, 100, 0.7)]: [0.65894417014812, 0.6662387676508345, 0.6889460154241646]\n",
      "(0.1, 0, 3, 0, 0.5, 100, 0.7): 0.6628178753669687\n",
      "[(0.01, 0, 3, 20, 0.5, 100, 0.7), (0.1, 0, 3, 0, 0.5, 100, 0.7), (0.2, 5, 10, 0, 0.5, 100, 0.7)]: [0.65894417014812, 0.6636713735558408, 0.6863753213367609]\n",
      "(0.01, 5, 3, 5, 1, 200, 1.0): 0.6612931371090863\n",
      "[(0.1, 1, 3, 5, 0.5, 100, 1.0), (0.01, 0, 3, 0, 1, 200, 1.0), (0.1, 1, 3, 5, 0.5, 400, 1.0)]: [0.6578047854158754, 0.6619597774925118, 0.6863753213367609]\n"
     ]
    }
   ],
   "source": [
    "#XGBClassifier hyperparameter tuning:\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "\n",
    "# Add season column\n",
    "games['season'] = games['game_date'].apply(assign_season)\n",
    "\n",
    "#each element of these two lists is itself a 3-element list, containing the best scores for each of the 3 train-test splits\n",
    "max_keys = []\n",
    "max_values = []\n",
    "best_keys_per_fold_and_setting = [] \n",
    "best_scores_per_fold_and_setting = []\n",
    "for i in [True, False]:\n",
    "    for j in list(range(0,3)):\n",
    "        max_key_folds = [0,0,0]\n",
    "        max_value_folds = [0,0,0]\n",
    "        \n",
    "        games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "        games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "    \n",
    "        # Add season column\n",
    "        games['season'] = games['game_date'].apply(assign_season)\n",
    "    \n",
    "        # Load the games DataFrame - drop relevant columns\n",
    "        games, wl_home = load_games_dataframe(games, j)\n",
    "        wl_home = wl_home.replace({'W': 1, 'L': 0})\n",
    "    \n",
    "        #Get the preprocessor - the i parameter decides if we use PCA or not.\n",
    "        preprocessor = get_preprocessor(games, \"XGB\", i)\n",
    "        \n",
    "        #Candidate parameter values\n",
    "        eta_list = [0.01, 0.1, 0.2]\n",
    "        gamma_list = [0, 1, 5]\n",
    "        max_depth_list = [3, 6, 10]\n",
    "        min_child_weight_list = [0, 5, 20]\n",
    "        subsample_list = [0.5, 1]\n",
    "        n_estimators_list = [100, 200, 400]\n",
    "        colsample_bytree_list = [0.7, 1.0]\n",
    "        \n",
    "        #Cartesian product of the lists\n",
    "        all_combinations = list(product(\n",
    "            eta_list,\n",
    "            gamma_list,\n",
    "            max_depth_list,\n",
    "            min_child_weight_list,\n",
    "            subsample_list,\n",
    "            n_estimators_list,\n",
    "            colsample_bytree_list,\n",
    "        ))\n",
    "                \n",
    "        # Create a copy of games since we're going to be changing the seasons column, and we don't want it affected for the other models.\n",
    "        games_xgb = games.copy()\n",
    "        \n",
    "        # Create a mapping dictionary\n",
    "        seasons = [f\"{year}-{year+1}\" for year in range(2013, 2025)]\n",
    "        season_mapping = {season: k for k, season in enumerate(seasons)}\n",
    "        \n",
    "        # Map the 'season' column to numeric values\n",
    "        games_xgb['season'] = games_xgb['season'].map(season_mapping)\n",
    "        \n",
    "        #Accuracy dict\n",
    "        accuracies = {combination : [] for combination in all_combinations}\n",
    "        avg_accuracies = {combination : 0 for combination in all_combinations}\n",
    "        \n",
    "        #Access individual elements of a combination and fit and train the model. Keep all of the scores, but especially the best score for each\n",
    "        #combination and each fold of the cross-validation.\n",
    "        for combination in all_combinations:\n",
    "            clear_output(wait=True)\n",
    "            print(combination)\n",
    "            eta = combination[0]       # First element\n",
    "            gamma = combination[1]  # Second element\n",
    "            max_depth = combination[2]   # Third element\n",
    "            min_child_weight = combination[3]       # Fourth element\n",
    "            subsample = combination[4]     # Fifth element\n",
    "            n_estimators = combination[5]\n",
    "            colsample_bytree = combination[6]\n",
    "            \n",
    "            classifier = XGBClassifier(eta=eta, gamma=gamma, max_depth=max_depth,\n",
    "                                    min_child_weight=min_child_weight, subsample=subsample, n_jobs=8, n_estimators=n_estimators,\n",
    "                                      colsample_bytree = colsample_bytree)\n",
    "            for k in list(range(0,3)):\n",
    "                data_train = games_xgb.loc[training_indices[k]]\n",
    "                target_train = wl_home.loc[training_indices[k]]\n",
    "                data_test = games_xgb.loc[testing_indices[k]]\n",
    "                target_test = wl_home.loc[testing_indices[k]]\n",
    "                data_train_processed = preprocessor.fit_transform(data_train)\n",
    "                data_test_processed = preprocessor.transform(data_test)\n",
    "                classifier.fit(data_train_processed, target_train)\n",
    "                score = classifier.score(data_test_processed, target_test)\n",
    "                accuracies[combination].append(score)\n",
    "                if score > max_value_folds[k]:\n",
    "                    max_value_folds[k] = score\n",
    "                    max_key_folds[k] = combination\n",
    "            avg_accuracies[combination] = statistics.mean(accuracies[combination])\n",
    "        accuracies_series = pd.Series(accuracies)\n",
    "        accuracies_series.to_csv(base_dir / (\"tuning_xgb_accuracies_\" + str(j) + \"-\" + str(i) + \".csv\"))\n",
    "        avg_accuracies_series = pd.Series(avg_accuracies)\n",
    "        avg_accuracies_series.to_csv(base_dir / (\"tuning_xgb_avg_accuracies_\" + str(j) + \"-\" + str(i) + \".csv\"))\n",
    "        max_key = max(avg_accuracies, key=avg_accuracies.get)\n",
    "        max_keys.append(max_key)\n",
    "        max_values.append(avg_accuracies[max_key])\n",
    "        best_keys_per_fold_and_setting.append(max_key_folds)\n",
    "        best_scores_per_fold_and_setting.append(max_value_folds)\n",
    "\n",
    "for i in range(0,6):\n",
    "    print(str(max_keys[i]) + \": \" + str(max_values[i]))\n",
    "    print(str(best_keys_per_fold_and_setting[i]) + \": \" + str(best_scores_per_fold_and_setting[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d691e1e-7969-47e4-a162-01a452096c82",
   "metadata": {},
   "source": [
    "## XGBoost Performance and Conclusion\n",
    "\n",
    "The best performing model, on average, was the one with PCA and all individual columns dropped (meaning we only kept the differences between the scores, not the individual scores themselves). Accuracy: 66.5%. The train-test split is the same as the one used for random forest.\n",
    "\n",
    "- Best performance on the first train-test split: no PCA, no columns dropped; accuracy 65.9%.\n",
    "- Second: no PCA, no columns dropped; accuracy 66.6%\n",
    "- Third: PCA, some columns dropped (kept the individual net ratings and individual player \"ODG\" ratings); accuracy 70.7%. High accuracy again probably due to very small test set.\n",
    "\n",
    "### RF-XGB Comparison\n",
    "\n",
    "The performances of the Random Forest model and the XGBoost model are very similar. The best average accuracy is slightly better with RF; on large test sets, RF performs slightly better. On a small test set, XGB performed significantly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57673b9e-230d-4554-bb99-52418fd4a581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19900\\3036384423.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - binary_accuracy: 0.6223 - loss: 4.1597 - val_binary_accuracy: 0.6310 - val_loss: 2.7299 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6553 - loss: 2.3877 - val_binary_accuracy: 0.6349 - val_loss: 1.6435 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6611 - loss: 1.4638 - val_binary_accuracy: 0.6381 - val_loss: 1.1107 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6700 - loss: 1.0085 - val_binary_accuracy: 0.6368 - val_loss: 0.8613 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6730 - loss: 0.7968 - val_binary_accuracy: 0.6357 - val_loss: 0.7484 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6691 - loss: 0.7019 - val_binary_accuracy: 0.6383 - val_loss: 0.6989 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6754 - loss: 0.6607 - val_binary_accuracy: 0.6357 - val_loss: 0.6788 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6704 - loss: 0.6472 - val_binary_accuracy: 0.6381 - val_loss: 0.6660 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6640 - loss: 0.6415 - val_binary_accuracy: 0.6373 - val_loss: 0.6626 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6653 - loss: 0.6372 - val_binary_accuracy: 0.6399 - val_loss: 0.6574 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6734 - loss: 0.6275 - val_binary_accuracy: 0.6365 - val_loss: 0.6566 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6643 - loss: 0.6341 - val_binary_accuracy: 0.6360 - val_loss: 0.6556 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6669 - loss: 0.6279 - val_binary_accuracy: 0.6373 - val_loss: 0.6549 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6600 - loss: 0.6322 - val_binary_accuracy: 0.6381 - val_loss: 0.6552 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6717 - loss: 0.6242 - val_binary_accuracy: 0.6375 - val_loss: 0.6585 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6610 - loss: 0.6376 - val_binary_accuracy: 0.6355 - val_loss: 0.6584 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6737 - loss: 0.6228 - val_binary_accuracy: 0.6399 - val_loss: 0.6574 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6710 - loss: 0.6278 - val_binary_accuracy: 0.6378 - val_loss: 0.6532 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6563 - loss: 0.6324 - val_binary_accuracy: 0.6360 - val_loss: 0.6562 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6719 - loss: 0.6238 - val_binary_accuracy: 0.6373 - val_loss: 0.6536 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6706 - loss: 0.6269 - val_binary_accuracy: 0.6401 - val_loss: 0.6542 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6646 - loss: 0.6280 - val_binary_accuracy: 0.6381 - val_loss: 0.6566 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6729 - loss: 0.6241 - val_binary_accuracy: 0.6386 - val_loss: 0.6537 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6627 - loss: 0.6284 - val_binary_accuracy: 0.6388 - val_loss: 0.6510 - learning_rate: 5.0000e-04\n",
      "Epoch 25/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6744 - loss: 0.6211 - val_binary_accuracy: 0.6394 - val_loss: 0.6506 - learning_rate: 5.0000e-04\n",
      "Epoch 26/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6684 - loss: 0.6243 - val_binary_accuracy: 0.6368 - val_loss: 0.6510 - learning_rate: 5.0000e-04\n",
      "Epoch 27/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6761 - loss: 0.6168 - val_binary_accuracy: 0.6375 - val_loss: 0.6511 - learning_rate: 5.0000e-04\n",
      "Epoch 28/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6725 - loss: 0.6205 - val_binary_accuracy: 0.6378 - val_loss: 0.6522 - learning_rate: 5.0000e-04\n",
      "Epoch 29/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6681 - loss: 0.6245 - val_binary_accuracy: 0.6412 - val_loss: 0.6511 - learning_rate: 5.0000e-04\n",
      "Epoch 30/5000\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6697 - loss: 0.6270 - val_binary_accuracy: 0.6368 - val_loss: 0.6538 - learning_rate: 5.0000e-04\n",
      "Epoch 31/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6677 - loss: 0.6278 - val_binary_accuracy: 0.6417 - val_loss: 0.6517 - learning_rate: 2.5000e-04\n",
      "Epoch 32/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6726 - loss: 0.6198 - val_binary_accuracy: 0.6383 - val_loss: 0.6502 - learning_rate: 2.5000e-04\n",
      "Epoch 33/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6670 - loss: 0.6233 - val_binary_accuracy: 0.6388 - val_loss: 0.6509 - learning_rate: 2.5000e-04\n",
      "Epoch 34/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6818 - loss: 0.6142 - val_binary_accuracy: 0.6427 - val_loss: 0.6506 - learning_rate: 2.5000e-04\n",
      "Epoch 35/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6656 - loss: 0.6230 - val_binary_accuracy: 0.6417 - val_loss: 0.6513 - learning_rate: 2.5000e-04\n",
      "Epoch 36/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6658 - loss: 0.6269 - val_binary_accuracy: 0.6412 - val_loss: 0.6507 - learning_rate: 2.5000e-04\n",
      "Epoch 37/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6729 - loss: 0.6193 - val_binary_accuracy: 0.6430 - val_loss: 0.6499 - learning_rate: 2.5000e-04\n",
      "Epoch 38/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6806 - loss: 0.6168 - val_binary_accuracy: 0.6414 - val_loss: 0.6514 - learning_rate: 2.5000e-04\n",
      "Epoch 39/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6742 - loss: 0.6143 - val_binary_accuracy: 0.6394 - val_loss: 0.6507 - learning_rate: 2.5000e-04\n",
      "Epoch 40/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6662 - loss: 0.6212 - val_binary_accuracy: 0.6414 - val_loss: 0.6502 - learning_rate: 2.5000e-04\n",
      "Epoch 41/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6709 - loss: 0.6190 - val_binary_accuracy: 0.6378 - val_loss: 0.6493 - learning_rate: 2.5000e-04\n",
      "Epoch 42/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6700 - loss: 0.6220 - val_binary_accuracy: 0.6409 - val_loss: 0.6520 - learning_rate: 2.5000e-04\n",
      "Epoch 43/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6779 - loss: 0.6136 - val_binary_accuracy: 0.6435 - val_loss: 0.6498 - learning_rate: 2.5000e-04\n",
      "Epoch 44/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6680 - loss: 0.6196 - val_binary_accuracy: 0.6425 - val_loss: 0.6499 - learning_rate: 2.5000e-04\n",
      "Epoch 45/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6691 - loss: 0.6181 - val_binary_accuracy: 0.6427 - val_loss: 0.6503 - learning_rate: 2.5000e-04\n",
      "Epoch 46/5000\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6705 - loss: 0.6219 - val_binary_accuracy: 0.6396 - val_loss: 0.6519 - learning_rate: 2.5000e-04\n",
      "Epoch 47/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6687 - loss: 0.6241 - val_binary_accuracy: 0.6407 - val_loss: 0.6493 - learning_rate: 1.2500e-04\n",
      "Epoch 48/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6704 - loss: 0.6166 - val_binary_accuracy: 0.6412 - val_loss: 0.6495 - learning_rate: 1.2500e-04\n",
      "Epoch 49/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6749 - loss: 0.6213 - val_binary_accuracy: 0.6396 - val_loss: 0.6497 - learning_rate: 1.2500e-04\n",
      "Epoch 50/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6731 - loss: 0.6173 - val_binary_accuracy: 0.6409 - val_loss: 0.6492 - learning_rate: 1.2500e-04\n",
      "Epoch 51/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6653 - loss: 0.6239 - val_binary_accuracy: 0.6417 - val_loss: 0.6500 - learning_rate: 1.2500e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6733 - loss: 0.6198 - val_binary_accuracy: 0.6409 - val_loss: 0.6499 - learning_rate: 1.2500e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6761 - loss: 0.6185 - val_binary_accuracy: 0.6396 - val_loss: 0.6491 - learning_rate: 1.2500e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6745 - loss: 0.6165 - val_binary_accuracy: 0.6404 - val_loss: 0.6494 - learning_rate: 1.2500e-04\n",
      "Epoch 55/5000\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6723 - loss: 0.6174 - val_binary_accuracy: 0.6407 - val_loss: 0.6494 - learning_rate: 1.2500e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6674 - loss: 0.6204 - val_binary_accuracy: 0.6422 - val_loss: 0.6496 - learning_rate: 6.2500e-05\n",
      "Epoch 57/5000\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6658 - loss: 0.6218 - val_binary_accuracy: 0.6412 - val_loss: 0.6498 - learning_rate: 6.2500e-05\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.6485 - loss: 0.6279 \n",
      "Test Accuracy: 0.66218\n"
     ]
    }
   ],
   "source": [
    "#load the data, use the maximum number of columns, so don't drop anything\n",
    "base_dir = Path.cwd()\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games = games.drop(columns = ['matchup_home'])\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
    "#drop a column to prevent data leakage\n",
    "games = games.drop(columns = ['plus_minus_home'])\n",
    "games['elo_diff'] = games['elo_home'] - games['elo_away']\n",
    "#games = games.drop(columns = ['elo_home', 'elo_away'])\n",
    "games['rating_diff'] = games['rating_home'] - games['rating_away']\n",
    "#games = games.drop(columns = ['rating_home', 'rating_away'])\n",
    "games = games.drop(columns = ['game_id'])\n",
    "for i in list(range(1,6)):\n",
    "    games['rating_player_diff' + str(i)] = games['rating_home_player' + str(i)] - games['rating_away_player' + str(i)]\n",
    "    #games = games.drop(columns = ['rating_home_player' + str(i), 'rating_away_player' + str(i)])\n",
    "games['home_netrtg'] = games['ortg_home'] - games['drtg_home']\n",
    "#games = games.drop(columns = ['ortg_home', 'drtg_home'])\n",
    "games['away_netrtg'] = games['ortg_away'] - games['drtg_away']\n",
    "#games = games.drop(columns = ['ortg_away', 'drtg_away'])\n",
    "games['home_netrtg_last_season'] = games['home_ortg_last_season'] - games['home_drtg_last_season']\n",
    "games['away_netrtg_last_season'] = games['away_ortg_last_season'] - games['away_drtg_last_season']\n",
    "#games = games.drop(columns = ['home_ortg_last_season', 'home_drtg_last_season', 'away_ortg_last_season', 'away_drtg_last_season'])\n",
    "games['GP_home'] = games['record_home_wins'] + games['record_home_losses']\n",
    "games['GP_away'] = games['record_away_wins'] + games['record_away_losses']\n",
    "#games = games.drop(columns = ['record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses'])\n",
    "games['wl%_diff'] = games['home_wl%'] - games['away_wl%']\n",
    "#games = games.drop(columns = ['home_wl%', 'away_wl%'])\n",
    "games['netrg_diff'] = games['home_netrtg'] - games['away_netrtg']\n",
    "#games = games.drop(columns = ['home_netrtg', 'away_netrtg'])\n",
    "games['netrtg_diff_last_season'] = games['home_netrtg_last_season'] - games['away_netrtg_last_season']\n",
    "#games = games.drop(columns = ['home_netrtg_last_season', 'away_netrtg_last_season'])\n",
    "games.replace('Charlotte Bobcats', 'Charlotte Hornets', inplace=True)\n",
    "games.replace('New Orleans Hornets', 'New Orleans Pelicans', inplace=True)\n",
    "games.to_csv(base_dir / \"games_differences.csv\")\n",
    "games_train = games[(games['game_date'] <= datetime.datetime(2023,9,30))]\n",
    "target_train = games_train['wl_home']\n",
    "data_train = games_train.drop(columns = ['wl_home'])\n",
    "games_test = games[games['game_date'] >= datetime.datetime(2023,10,1)]\n",
    "target_test = games_test['wl_home']\n",
    "data_test = games_test.drop(columns = ['wl_home'])\n",
    "\n",
    "\n",
    "#preprocessing - turning dates into days since first game in table and one-hot encoding team names\n",
    "#now we also rescale the data since this is necessary for neural networks!\n",
    "data_train['game_date'] = (data_train['game_date'] - data_train['game_date'].min()) / pd.Timedelta(days=1)\n",
    "data_test['game_date'] = (data_test['game_date'] - data_test['game_date'].min()) / pd.Timedelta(days=1)\n",
    "categorical_columns = ['team_name_home', 'team_name_away', 'season_type']\n",
    "numerical_columns = [col for col in data_train.columns if col not in categorical_columns]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('team_names', OneHotEncoder(), categorical_columns),  # Apply OneHotEncoder to team columns\n",
    "        ('numerical_for_rescaling', StandardScaler(), numerical_columns),\n",
    "    ],\n",
    ")\n",
    "data_train_processed = preprocessor.fit_transform(data_train)\n",
    "data_test_processed = preprocessor.transform(data_test)\n",
    "\n",
    "#add early stopping and learning rate scheduler to reduce overfitting!\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "#define and compile the neural network\n",
    "model_nn = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu',input_shape=[data_train_processed.shape[1]], kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "model_nn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "#finally, train and test the neural network\n",
    "history = model_nn.fit(data_train_processed, target_train,\n",
    "                       validation_split=0.2,  # Use a portion of training data for validation\n",
    "                       epochs=5000,            # Adjust epochs as needed\n",
    "                       batch_size=256,        # Mini-batch size\n",
    "                       callbacks = [early_stopping, lr_scheduler],\n",
    "                       verbose=1)            # Set verbose to 1 to see the training process\n",
    "test_loss, test_accuracy = model_nn.evaluate(data_test_processed, target_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3b32168-e8ae-4972-a3ae-f94aa709516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_neural_network(pca_arg, drop_columns, lr, optimizer, batch_size, n_layers, initial_layer_neurons, activation, dropout_rate, training_indices, testing_indices):\n",
    "    games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "    games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "    \n",
    "    # Add season column\n",
    "    games['season'] = games['game_date'].apply(assign_season)\n",
    "\n",
    "    # Load the games DataFrame - drop relevant columns\n",
    "    games, wl_home = load_games_dataframe(games, drop_columns)\n",
    "    wl_home = wl_home.replace({'W': 1, 'L': 0})\n",
    "    \n",
    "    #Get the preprocessor - the pca_arg parameter decides if we use PCA or not.\n",
    "    preprocessor = get_preprocessor(games, \"NN\", pca_arg)\n",
    "            \n",
    "    # Create a mapping dictionary\n",
    "    seasons = [f\"{year}-{year+1}\" for year in range(2013, 2025)]\n",
    "    season_mapping = {season: k for k, season in enumerate(seasons)}\n",
    "        \n",
    "    # Map the 'season' column to numeric values\n",
    "    games['season'] = games['season'].map(season_mapping)\n",
    "\n",
    "    #add early stopping and learning rate scheduler to reduce overfitting!\n",
    "    early_stopping = EarlyStopping(\n",
    "        min_delta = 0.001,\n",
    "        patience = 20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "    )\n",
    "\n",
    "    test_accuracy = [0,0,0]\n",
    "    \n",
    "    for k in list(range(0,3)):\n",
    "        #load the training and testing sets (use the same train-test split as for RF and XGB)\n",
    "        data_train = games.loc[training_indices[k]]\n",
    "        target_train = wl_home.loc[training_indices[k]]\n",
    "        data_test = games.loc[testing_indices[k]]\n",
    "        target_test = wl_home.loc[testing_indices[k]]\n",
    "        #preprocess the train and test sets\n",
    "        data_train_processed = preprocessor.fit_transform(data_train)\n",
    "        data_test_processed = preprocessor.transform(data_test)\n",
    "        #define the model; use the initial_layer_neurons parameter for the first layer, then divide it by 2 for each subsequent layer.\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(initial_layer_neurons, activation=activation, input_shape=[data_train_processed.shape[1]], kernel_regularizer=l2(0.01)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        n_layers -= 1\n",
    "        initial_layer_neurons //= 2\n",
    "        while n_layers > 0:\n",
    "            model.add(layers.Dense(initial_layer_neurons, activation=activation, kernel_regularizer=l2(0.01)))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            n_layers -= 1\n",
    "            initial_layer_neurons //= 2\n",
    "        #finally, add the output layer with sigmoid activation\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        #raise exception if optimizer parameter is wrong\n",
    "        if optimizer not in ['adam', 'SGD']:\n",
    "            raise ValueError(\"The optimizer must be either 'adam' or 'SGD'!\")\n",
    "        if optimizer == 'adam':\n",
    "            my_optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        else : my_optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "        #compile and fit the model\n",
    "        model.compile(\n",
    "        optimizer = my_optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy'],\n",
    "        )\n",
    "        \n",
    "        history = model.fit(data_train_processed, target_train,\n",
    "                       validation_split=0.2,  # Use a portion of training data for validation\n",
    "                       epochs=5000,            # Adjust epochs as needed\n",
    "                       batch_size=batch_size,        # Mini-batch size\n",
    "                       callbacks = [early_stopping, lr_scheduler],\n",
    "                       verbose=0)\n",
    "        test_loss, test_accuracy[k] = model.evaluate(data_test_processed, target_test)\n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84666dee-266a-4a8e-9383-bf1a4d65eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 'adam', 256, 5, 'swish', 0.4)\n",
      "(11503, 85)\n",
      "(11503,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13072\\438362709.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  wl_home = wl_home.replace({'W': 1, 'L': 0})\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.5858 - loss: 0.6802 \n",
      "(12187, 85)\n",
      "(12187,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.6456 - loss: 0.6220\n",
      "(14136, 85)\n",
      "(14136,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.6425 - loss: 0.6305 \n",
      "(11503, 85)\n",
      "(11503,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13072\\438362709.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  wl_home = wl_home.replace({'W': 1, 'L': 0})\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.5858 - loss: 0.6807 \n",
      "(12187, 85)\n",
      "(12187,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0003906250058207661.\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00019531250291038305.\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.6361 - loss: 0.6221 \n",
      "(14136, 85)\n",
      "(14136,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0003906250058207661.\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6530 - loss: 0.6320 \n",
      "(0.001, 'adam', 32, 3, 'relu', 0.4, 64): 0.6608890493710836\n",
      "[(0.1, 'adam', 256, 3, 'relu', 0.4, 64), (0.01, 'adam', 32, 3, 'relu', 0.4, 256), (0.001, 'adam', 32, 3, 'relu', 0.4, 64)]: [0.6543866395950317, 0.6615318655967712, 0.6812339425086975]\n",
      "(0.001, 'adam', 32, 3, 'relu', 0.4, 256): 0.662771463394165\n",
      "[(0.001, 'adam', 256, 3, 'swish', 0.4, 256), (0.001, 'adam', 32, 3, 'relu', 0.4, 256), (0.1, 'adam', 256, 5, 'swish', 0.4, 512)]: [0.651728093624115, 0.6658108830451965, 0.6812339425086975]\n",
      "(0.001, 'adam', 256, 3, 'relu', 0.4, 128): 0.6599309245745341\n",
      "[(0.01, 'adam', 32, 5, 'relu', 0.4, 256), (0.1, 'adam', 256, 5, 'relu', 0.4, 256), (0.01, 'adam', 32, 5, 'relu', 0.4, 512)]: [0.6551462411880493, 0.665382981300354, 0.6812339425086975]\n",
      "(0.01, 'adam', 32, 3, 'relu', 0.4, 128): 0.6626973549524943\n",
      "[(0.01, 'adam', 256, 3, 'swish', 0.4, 256), (0.001, 'adam', 32, 3, 'relu', 0.4, 64), (0.01, 'adam', 32, 3, 'relu', 0.4, 128)]: [0.6543866395950317, 0.6636713743209839, 0.6863753199577332]\n",
      "(0.001, 'adam', 256, 3, 'relu', 0.4, 128): 0.6637261907259623\n",
      "[(0.01, 'adam', 256, 5, 'swish', 0.4, 512), (0.1, 'adam', 32, 3, 'relu', 0.4, 256), (0.001, 'adam', 256, 3, 'relu', 0.4, 128)]: [0.6524876356124878, 0.6649550795555115, 0.6863753199577332]\n",
      "(0.001, 'adam', 256, 3, 'relu', 0.4, 128): 0.6644061605135599\n",
      "[(0.001, 'adam', 256, 3, 'swish', 0.4, 64), (0.001, 'adam', 256, 3, 'relu', 0.4, 128), (0.001, 'adam', 256, 3, 'relu', 0.4, 128)]: [0.6509684920310974, 0.6611039638519287, 0.6838046312332153]\n"
     ]
    }
   ],
   "source": [
    "#Tuning the Neural Network!\n",
    "#each element of these 4 lists is itself a 3-element list, containing the best scores for each of the 3 train-test splits and their resp. keys\n",
    "max_keys = []\n",
    "max_values = []\n",
    "best_keys_per_fold_and_setting = [] \n",
    "best_scores_per_fold_and_setting = []\n",
    "for i in [True, False]:\n",
    "    for j in list(range(0,3)):\n",
    "        max_key_folds = [0,0,0]\n",
    "        max_value_folds = [0,0,0]\n",
    "\n",
    "        #Candidate parameter values\n",
    "        lr_list = [1e-3, 1e-2, 0.1]\n",
    "        optimizer_list = ['adam']\n",
    "        batch_size_list = [32, 256]\n",
    "        no_of_layers_list = [3, 5]\n",
    "        activation_list = ['relu', 'swish']\n",
    "        dropout_rate_list = [0.4]\n",
    "        \n",
    "        #Cartesian product of the lists\n",
    "        all_combinations = list(product(\n",
    "            lr_list,\n",
    "            optimizer_list,\n",
    "            batch_size_list,\n",
    "            no_of_layers_list,\n",
    "            activation_list,\n",
    "            dropout_rate_list,\n",
    "        ))\n",
    "\n",
    "        #Accuracy dict\n",
    "        accuracies = {}\n",
    "        avg_accuracies = {}\n",
    "        \n",
    "        #Access individual elements of a combination and fit and train the modelfor that combination.\n",
    "        #Keep all of the scores, but especially the best score for each\n",
    "        #combination and each fold of the cross-validation.\n",
    "        #Compute the best average scores in the end.\n",
    "        for combination in all_combinations:\n",
    "            clear_output(wait=True)\n",
    "            print(combination)\n",
    "            lr = combination[0]       # First element\n",
    "            optimizer = combination[1]  # Second element\n",
    "            batch_size = combination[2]   # Third element\n",
    "            no_of_layers = combination[3]       # Fourth element\n",
    "            activation = combination[4]     # Fifth element\n",
    "            dropout_rate = combination[5]\n",
    "            if no_of_layers == 3:\n",
    "                for initial_layer_neurons in [64, 128, 256]:\n",
    "                    new_combo = combination + (initial_layer_neurons,)\n",
    "                    accuracies[new_combo] = tune_neural_network(i, j, lr, optimizer, batch_size, no_of_layers, initial_layer_neurons, activation, dropout_rate, training_indices, testing_indices)\n",
    "                    for k in list(range(3)):\n",
    "                        if accuracies[new_combo][k] > max_value_folds[k]:\n",
    "                            max_value_folds[k] = accuracies[new_combo][k]\n",
    "                            max_key_folds[k] = new_combo\n",
    "                    avg_accuracies[new_combo] = statistics.mean(accuracies[new_combo])\n",
    "            if no_of_layers == 5:\n",
    "                for initial_layer_neurons in [256, 512]:\n",
    "                    new_combo = combination + (initial_layer_neurons,)\n",
    "                    accuracies[new_combo] = tune_neural_network(i, j, lr, optimizer, batch_size, no_of_layers, initial_layer_neurons, activation, dropout_rate, training_indices, testing_indices)\n",
    "                    for k in list(range(3)):\n",
    "                        if accuracies[new_combo][k] > max_value_folds[k]:\n",
    "                            max_value_folds[k] = accuracies[new_combo][k]\n",
    "                            max_key_folds[k] = new_combo\n",
    "                    avg_accuracies[new_combo] = statistics.mean(accuracies[new_combo])\n",
    "        accuracies_series = pd.Series(accuracies)\n",
    "        accuracies_series.to_csv(base_dir / (\"tuning_nn_accuracies_\" + str(j) + \"-\" + str(i) + \".csv\"))\n",
    "        avg_accuracies_series = pd.Series(avg_accuracies)\n",
    "        avg_accuracies_series.to_csv(base_dir / (\"tuning_nn_avg_accuracies_\" + str(j) + \"-\" + str(i) + \".csv\"))\n",
    "        max_key = max(avg_accuracies, key=avg_accuracies.get)\n",
    "        max_keys.append(max_key)\n",
    "        max_values.append(avg_accuracies[max_key])\n",
    "        best_keys_per_fold_and_setting.append(max_key_folds)\n",
    "        best_scores_per_fold_and_setting.append(max_value_folds)\n",
    "\n",
    "for i in range(0,6):\n",
    "    print(str(max_keys[i]) + \": \" + str(max_values[i]))\n",
    "    print(str(best_keys_per_fold_and_setting[i]) + \": \" + str(best_scores_per_fold_and_setting[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763eb8c6-e2f4-44fe-8a21-39d3861af219",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Neural Network Performance\n",
    "\n",
    "On average, the neural network model performed best without PCA and with all individual columns dropped. The best parameters were the following: learning_rate = 0.001, batch_size = 256, 3 hidden layers with 128/64/32 neurons, 'relu' activation function. Average accuracy: 66.4%.\n",
    "\n",
    "- Best performance on the first train-test split: with PCA, all individual columns dropped. learning_rate = 0.01, batch_size = 32, 5 hidden layers with 256/128/64/32/16 neurons, 'relu' activation function. Accuracy: 65.6%\n",
    "- Best performance on the second train-test split: with PCA, most individual columnns dropped. learning_rate = 0.001, batch_size = 32, 3 hidden layers with 256/128/64 neurons, 'relu' activation function. Accuracy: 66.6%\n",
    "- Best performance on the third train-test split: joint between:\n",
    "   1) without PCA, no individual columns dropped. learning_rate = 0.01, batch_size = 32, 3 hidden layers with 128/64/32 neurons, 'relu' activation function, and\n",
    "   2) without PCA, most individual columns dropped. learning_rate = 0.001, batch_size = 256, 3 hidden layers with 128/64/32 neurons, 'relu' acrivation function.\n",
    "Accuracy of both: 68.6%\n",
    "\n",
    "The neural network model performed slightly worse than the RF and XGB models. This could be due to the fact that the parameter grid for tuning had less elements than for the RF and XGB models, which is itself due tot he fact that the NN model takes longer to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e440c-b8ce-49c2-b9f9-6a733f573ff7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All three models displayed very similar performance, with RF and XGB slightly ahead of NN. Hyperparameter tuning did not cause dramatic improvement. The best performance was by far on the third train-test split, which is possibly due to the very small test set (389 games compared to 1500-2000 in the first two), which causes higher variance of accuracy w/r/t change in hyperparameter values.\n",
    "\n",
    "My main hope is that the next jump in model performance could be achieved by measuring teams' scores only on their last 10/20/30 games. Although a decent indicator of a player's quality, performance over a full season, and especially over the last 2/3/5 seasons is probably too long a period to make near-future predictions.\n",
    "\n",
    "NB: Below are initial versions of the models. Probably not worth looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61a103c-a7d2-4787-b9c2-7eae8f60fb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'game_id', 'game_date', 'team_name_home',\n",
       "       'team_name_away', 'matchup_home', 'wl_home', 'record_home_wins',\n",
       "       'record_home_losses', 'record_away_wins', 'record_away_losses',\n",
       "       'elo_home', 'elo_away', 'plus_minus_home', 'season_type', 'home_wl%',\n",
       "       'away_wl%', 'rating_home', 'rating_away', 'rating_home_player1',\n",
       "       'rating_away_player1', 'rating_home_player2', 'rating_away_player2',\n",
       "       'rating_home_player3', 'rating_away_player3', 'rating_home_player4',\n",
       "       'rating_away_player4', 'rating_home_player5', 'rating_away_player5',\n",
       "       'ortg_home', 'drtg_home', 'ortg_away', 'drtg_away',\n",
       "       'home_ortg_last_season', 'home_drtg_last_season',\n",
       "       'away_ortg_last_season', 'away_drtg_last_season'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33046bdd-c5be-4642-bfd4-7da3843f1c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1736/2633, 0.6593239650588683\n",
      "1562/2337, 0.6683782627299957\n",
      "256/389, 0.6580976863753213\n"
     ]
    }
   ],
   "source": [
    "length_test_set = 0\n",
    "correct_predictions = 0\n",
    "for k in list(range(0,3)):\n",
    "    data_test = games.loc[testing_indices[k]]\n",
    "    length_test_set = len(data_test)\n",
    "    correct_predictions = len(data_test.loc[((data_test['wl_home']=='W') & (data_test['rating_home']+3 >= data_test['rating_away'])) | ((data_test['wl_home']=='L') & (data_test['rating_home']+3 < data_test['rating_away']))])\n",
    "    print(str(correct_predictions) + \"/\" + str(length_test_set) + \", \" + str(float(correct_predictions)/float(length_test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5cb2d2-80e2-41d2-8352-21e781b2a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6428\\4171052238.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6593225006885156, 0.6356375654089782, 0.6518865326356376],\n",
       " [[array([   0,    1,    2, ..., 3629, 3630, 3631]),\n",
       "   array([3632, 3633, 3634, ..., 7260, 7261, 7262])],\n",
       "  [array([   0,    1,    2, ..., 7260, 7261, 7262]),\n",
       "   array([ 7263,  7264,  7265, ..., 10891, 10892, 10893])],\n",
       "  [array([    0,     1,     2, ..., 10891, 10892, 10893]),\n",
       "   array([10894, 10895, 10896, ..., 14522, 14523, 14524])]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest with time series cross-validation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "#load the data\n",
    "base_dir = Path.cwd()\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games = games.drop(columns = ['matchup_home'])\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
    "#drop a column to prevent data leakage\n",
    "games = games.drop(columns = ['plus_minus_home'])\n",
    "#add the difference in Elo ratings as a column, drop the individual Elo ratings\n",
    "games['elo_diff'] = games['elo_home'] - games['elo_away']\n",
    "games = games.drop(columns = ['elo_home', 'elo_away'])\n",
    "#add the gamescore-offrtg-defrtg rating difference as a column, drop the individual team ratings\n",
    "games['rating_diff'] = games['rating_home'] - games['rating_away']\n",
    "games = games.drop(columns = ['rating_home', 'rating_away'])\n",
    "#drop game_id, it's just an index column\n",
    "games = games.drop(columns = ['game_id'])\n",
    "#add the rating *difference* between the nth best players, but don't drop the individual ratings\n",
    "for i in list(range(1,6)):\n",
    "    games['rating_player_diff' + str(i)] = games['rating_home_player' + str(i)] - games['rating_away_player' + str(i)]\n",
    "    #games = games.drop(columns = ['rating_home_player' + str(i), 'rating_away_player' + str(i)])\n",
    "#add home net rating instead of having both offrtg and defrtg\n",
    "games['home_netrtg'] = games['ortg_home'] - games['drtg_home']\n",
    "games = games.drop(columns = ['ortg_home', 'drtg_home'])\n",
    "#add away net rating...\n",
    "games['away_netrtg'] = games['ortg_away'] - games['drtg_away']\n",
    "games = games.drop(columns = ['ortg_away', 'drtg_away'])\n",
    "#add home net rating and away net rating from last season\n",
    "games['home_netrtg_last_season'] = games['home_ortg_last_season'] - games['home_drtg_last_season']\n",
    "games['away_netrtg_last_season'] = games['away_ortg_last_season'] - games['away_drtg_last_season']\n",
    "games = games.drop(columns = ['home_ortg_last_season', 'home_drtg_last_season', 'away_ortg_last_season', 'away_drtg_last_season'])\n",
    "#add games played home/away (games played so far this season)\n",
    "games['GP_home'] = games['record_home_wins'] + games['record_home_losses']\n",
    "games['GP_away'] = games['record_away_wins'] + games['record_away_losses']\n",
    "games = games.drop(columns = ['record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses'])\n",
    "#add difference in win-loss percentage, remove individual win-loss percentages\n",
    "games['wl%_diff'] = games['home_wl%'] - games['away_wl%']\n",
    "games = games.drop(columns = ['home_wl%', 'away_wl%'])\n",
    "#add net rating difference, but keep individual home/away net ratings - apparently the model works better with them kept in\n",
    "games['netrg_diff'] = games['home_netrtg'] - games['away_netrtg']\n",
    "#games = games.drop(columns = ['home_netrtg', 'away_netrtg'])\n",
    "#add netrtg difference from last season, drop individual net rtgs\n",
    "games['netrtg_diff_last_season'] = games['home_netrtg_last_season'] - games['away_netrtg_last_season']\n",
    "games = games.drop(columns = ['home_netrtg_last_season', 'away_netrtg_last_season'])\n",
    "#in 2013-14, the charlotte hornets were called the bobcats, and (maybe) the new orleans pelicans were called the hornets\n",
    "games.replace('Charlotte Bobcats', 'Charlotte Hornets', inplace=True)\n",
    "games.replace('New Orleans Hornets', 'New Orleans Pelicans', inplace=True)\n",
    "\n",
    "#preprocessing - turning dates into days since first game in table and one-hot encoding team names\n",
    "games['game_date'] = (games['game_date'] - games['game_date'].min()) / pd.Timedelta(days=1)\n",
    "categorical_columns = ['team_name_home', 'team_name_away', 'season_type']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('team_names', OneHotEncoder(), categorical_columns)  # Apply OneHotEncoder to team columns\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns as is (e.g., numeric columns)\n",
    ")\n",
    "\n",
    "\n",
    "#fit and test the model - the n_estimators was tuned - other parameters gave the best results with the default values\n",
    "classifier = RandomForestClassifier(n_estimators = 400, n_jobs = 2, random_state = 0)\n",
    "model = make_pipeline(preprocessor, classifier)\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "indices = []\n",
    "accuracies = []\n",
    "for train_index, test_index in tscv.split(games):\n",
    "    indices.append([train_index, test_index])\n",
    "    data_train = games.iloc[train_index]\n",
    "    target_train = data_train['wl_home']\n",
    "    data_train = data_train.drop(columns = ['wl_home'])\n",
    "    data_test = games.iloc[test_index]\n",
    "    target_test = data_test['wl_home']\n",
    "    data_test = data_test.drop(columns = ['wl_home'])\n",
    "    model.fit(data_train, target_train)\n",
    "    accuracies.append(model.score(data_test, target_test))\n",
    "accuracies, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3037aabe-acb8-457b-ba06-e9bf9bb3d2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6428\\2113792585.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6565684384467089, 0.619388598182319, 0.6480308454971082],\n",
       " [[array([   0,    1,    2, ..., 3629, 3630, 3631]),\n",
       "   array([3632, 3633, 3634, ..., 7260, 7261, 7262])],\n",
       "  [array([   0,    1,    2, ..., 7260, 7261, 7262]),\n",
       "   array([ 7263,  7264,  7265, ..., 10891, 10892, 10893])],\n",
       "  [array([    0,     1,     2, ..., 10891, 10892, 10893]),\n",
       "   array([10894, 10895, 10896, ..., 14522, 14523, 14524])]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest for\n",
    "\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from pathlib import Path\n",
    "from xgboost import XGBClassifier\n",
    "import datetime\n",
    "\n",
    "#load the data\n",
    "base_dir = Path.cwd()\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games = games.drop(columns = ['matchup_home'])\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
    "#drop a column to prevent data leakage\n",
    "games = games.drop(columns = ['plus_minus_home'])\n",
    "#add the difference in Elo ratings as a column, drop the individual Elo ratings\n",
    "games['elo_diff'] = games['elo_home'] - games['elo_away']\n",
    "games = games.drop(columns = ['elo_home', 'elo_away'])\n",
    "#add the gamescore-offrtg-defrtg rating difference as a column, drop the individual team ratings\n",
    "games['rating_diff'] = games['rating_home'] - games['rating_away']\n",
    "games = games.drop(columns = ['rating_home', 'rating_away'])\n",
    "#drop game_id, it's just an index column\n",
    "games = games.drop(columns = ['game_id'])\n",
    "#add the rating *difference* between the nth best players, but don't drop the individual ratings\n",
    "for i in list(range(1,6)):\n",
    "    games['rating_player_diff' + str(i)] = games['rating_home_player' + str(i)] - games['rating_away_player' + str(i)]\n",
    "    #games = games.drop(columns = ['rating_home_player' + str(i), 'rating_away_player' + str(i)])\n",
    "#add home net rating instead of having both offrtg and defrtg\n",
    "games['home_netrtg'] = games['ortg_home'] - games['drtg_home']\n",
    "games = games.drop(columns = ['ortg_home', 'drtg_home'])\n",
    "#add away net rating...\n",
    "games['away_netrtg'] = games['ortg_away'] - games['drtg_away']\n",
    "games = games.drop(columns = ['ortg_away', 'drtg_away'])\n",
    "#add home net rating and away net rating from last season\n",
    "games['home_netrtg_last_season'] = games['home_ortg_last_season'] - games['home_drtg_last_season']\n",
    "games['away_netrtg_last_season'] = games['away_ortg_last_season'] - games['away_drtg_last_season']\n",
    "games = games.drop(columns = ['home_ortg_last_season', 'home_drtg_last_season', 'away_ortg_last_season', 'away_drtg_last_season'])\n",
    "#add games played home/away (games played so far this season)\n",
    "games['GP_home'] = games['record_home_wins'] + games['record_home_losses']\n",
    "games['GP_away'] = games['record_away_wins'] + games['record_away_losses']\n",
    "games = games.drop(columns = ['record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses'])\n",
    "#add difference in win-loss percentage, remove individual win-loss percentages\n",
    "games['wl%_diff'] = games['home_wl%'] - games['away_wl%']\n",
    "games = games.drop(columns = ['home_wl%', 'away_wl%'])\n",
    "#add net rating difference, but keep individual home/away net ratings - apparently the model works better with them kept in\n",
    "games['netrg_diff'] = games['home_netrtg'] - games['away_netrtg']\n",
    "#games = games.drop(columns = ['home_netrtg', 'away_netrtg'])\n",
    "#add netrtg difference from last season, drop individual net rtgs\n",
    "games['netrtg_diff_last_season'] = games['home_netrtg_last_season'] - games['away_netrtg_last_season']\n",
    "games = games.drop(columns = ['home_netrtg_last_season', 'away_netrtg_last_season'])\n",
    "#in 2013-14, the charlotte hornets were called the bobcats, and (maybe) the new orleans pelicans were called the hornets\n",
    "games.replace('Charlotte Bobcats', 'Charlotte Hornets', inplace=True)\n",
    "games.replace('New Orleans Hornets', 'New Orleans Pelicans', inplace=True)\n",
    "\n",
    "#preprocessing - turning dates into days since first game in table and one-hot encoding team names\n",
    "games['game_date'] = (games['game_date'] - games['game_date'].min()) / pd.Timedelta(days=1)\n",
    "categorical_columns = ['team_name_home', 'team_name_away', 'season_type']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('team_names', OneHotEncoder(), categorical_columns)  # Apply OneHotEncoder to team columns\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns as is (e.g., numeric columns)\n",
    ")\n",
    "\n",
    "\n",
    "#fit and test the model - the n_estimators was tuned - other parameters gave the best results with the default values\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "indices = []\n",
    "accuracies = []\n",
    "for train_index, test_index in tscv.split(games):\n",
    "    indices.append([train_index, test_index])\n",
    "    data_train = games.iloc[train_index]\n",
    "    target_train = data_train['wl_home']\n",
    "    data_train = data_train.drop(columns = ['wl_home'])\n",
    "    data_test = games.iloc[test_index]\n",
    "    target_test = data_test['wl_home']\n",
    "    data_test = data_test.drop(columns = ['wl_home'])\n",
    "    #now use xgboost on he same data...\n",
    "    data_train_processed = preprocessor.fit_transform(data_train)\n",
    "    data_test_processed = preprocessor.transform(data_test)\n",
    "    model_xgb = XGBClassifier(random_state=0, learning_rate=0.01, n_estimators=200)\n",
    "    model_xgb.fit(data_train_processed, target_train)\n",
    "    accuracies.append(model_xgb.score(data_test_processed, target_test))\n",
    "accuracies, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d5636c-2a52-4492-a265-01adfd698aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6428\\2024839660.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - binary_accuracy: 0.5274 - loss: 4.4111 - val_binary_accuracy: 0.6919 - val_loss: 3.7519 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6398 - loss: 3.6070 - val_binary_accuracy: 0.6795 - val_loss: 3.0843 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6632 - loss: 2.9749 - val_binary_accuracy: 0.6809 - val_loss: 2.5420 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6635 - loss: 2.4554 - val_binary_accuracy: 0.6795 - val_loss: 2.1038 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6594 - loss: 2.0417 - val_binary_accuracy: 0.6823 - val_loss: 1.7557 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6740 - loss: 1.7007 - val_binary_accuracy: 0.6823 - val_loss: 1.4838 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6721 - loss: 1.4440 - val_binary_accuracy: 0.6823 - val_loss: 1.2737 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.6727 - loss: 1.2556 - val_binary_accuracy: 0.6823 - val_loss: 1.1122 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6748 - loss: 1.1002 - val_binary_accuracy: 0.6836 - val_loss: 0.9872 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6840 - loss: 0.9777 - val_binary_accuracy: 0.6850 - val_loss: 0.8935 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6663 - loss: 0.8903 - val_binary_accuracy: 0.6864 - val_loss: 0.8239 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6787 - loss: 0.8287 - val_binary_accuracy: 0.6809 - val_loss: 0.7702 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6806 - loss: 0.7760 - val_binary_accuracy: 0.6905 - val_loss: 0.7302 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6827 - loss: 0.7421 - val_binary_accuracy: 0.6850 - val_loss: 0.6995 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6798 - loss: 0.7076 - val_binary_accuracy: 0.6864 - val_loss: 0.6764 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6737 - loss: 0.6932 - val_binary_accuracy: 0.6878 - val_loss: 0.6612 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6802 - loss: 0.6705 - val_binary_accuracy: 0.6850 - val_loss: 0.6472 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6723 - loss: 0.6632 - val_binary_accuracy: 0.6850 - val_loss: 0.6390 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6678 - loss: 0.6581 - val_binary_accuracy: 0.6864 - val_loss: 0.6312 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6769 - loss: 0.6391 - val_binary_accuracy: 0.6891 - val_loss: 0.6246 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6783 - loss: 0.6374 - val_binary_accuracy: 0.6905 - val_loss: 0.6229 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6872 - loss: 0.6294 - val_binary_accuracy: 0.6933 - val_loss: 0.6181 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6831 - loss: 0.6242 - val_binary_accuracy: 0.6864 - val_loss: 0.6158 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6683 - loss: 0.6360 - val_binary_accuracy: 0.6836 - val_loss: 0.6132 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6776 - loss: 0.6281 - val_binary_accuracy: 0.6878 - val_loss: 0.6122 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6723 - loss: 0.6271 - val_binary_accuracy: 0.6850 - val_loss: 0.6135 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6647 - loss: 0.6275 - val_binary_accuracy: 0.6891 - val_loss: 0.6106 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6696 - loss: 0.6320 - val_binary_accuracy: 0.6878 - val_loss: 0.6099 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6792 - loss: 0.6185 - val_binary_accuracy: 0.6878 - val_loss: 0.6103 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6749 - loss: 0.6272 - val_binary_accuracy: 0.6878 - val_loss: 0.6107 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6862 - loss: 0.6223 - val_binary_accuracy: 0.6850 - val_loss: 0.6090 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6902 - loss: 0.6178 - val_binary_accuracy: 0.6850 - val_loss: 0.6084 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6789 - loss: 0.6244 - val_binary_accuracy: 0.6850 - val_loss: 0.6076 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6797 - loss: 0.6176 - val_binary_accuracy: 0.6878 - val_loss: 0.6072 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6781 - loss: 0.6275 - val_binary_accuracy: 0.6878 - val_loss: 0.6084 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.6713 - loss: 0.6260 - val_binary_accuracy: 0.6864 - val_loss: 0.6063 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6801 - loss: 0.6166 - val_binary_accuracy: 0.6864 - val_loss: 0.6097 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6821 - loss: 0.6198 - val_binary_accuracy: 0.6905 - val_loss: 0.6089 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6763 - loss: 0.6238 - val_binary_accuracy: 0.6946 - val_loss: 0.6083 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6733 - loss: 0.6151 - val_binary_accuracy: 0.6878 - val_loss: 0.6068 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6700 - loss: 0.6256 - val_binary_accuracy: 0.6891 - val_loss: 0.6064 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6746 - loss: 0.6194 - val_binary_accuracy: 0.6878 - val_loss: 0.6059 - learning_rate: 5.0000e-04\n",
      "Epoch 43/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6775 - loss: 0.6162 - val_binary_accuracy: 0.6905 - val_loss: 0.6065 - learning_rate: 5.0000e-04\n",
      "Epoch 44/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6740 - loss: 0.6221 - val_binary_accuracy: 0.6919 - val_loss: 0.6056 - learning_rate: 5.0000e-04\n",
      "Epoch 45/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6848 - loss: 0.6187 - val_binary_accuracy: 0.6878 - val_loss: 0.6050 - learning_rate: 5.0000e-04\n",
      "Epoch 46/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6781 - loss: 0.6142 - val_binary_accuracy: 0.6919 - val_loss: 0.6071 - learning_rate: 5.0000e-04\n",
      "Epoch 47/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6755 - loss: 0.6252 - val_binary_accuracy: 0.6891 - val_loss: 0.6064 - learning_rate: 5.0000e-04\n",
      "Epoch 48/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6728 - loss: 0.6240 - val_binary_accuracy: 0.6905 - val_loss: 0.6047 - learning_rate: 5.0000e-04\n",
      "Epoch 49/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6750 - loss: 0.6166 - val_binary_accuracy: 0.6891 - val_loss: 0.6056 - learning_rate: 5.0000e-04\n",
      "Epoch 50/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6803 - loss: 0.6179 - val_binary_accuracy: 0.6919 - val_loss: 0.6063 - learning_rate: 5.0000e-04\n",
      "Epoch 51/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6697 - loss: 0.6217 - val_binary_accuracy: 0.6891 - val_loss: 0.6066 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6908 - loss: 0.6112 - val_binary_accuracy: 0.6891 - val_loss: 0.6063 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6919 - loss: 0.6082 - val_binary_accuracy: 0.6919 - val_loss: 0.6060 - learning_rate: 5.0000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6850 - loss: 0.6124 - val_binary_accuracy: 0.6905 - val_loss: 0.6069 - learning_rate: 2.5000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6717 - loss: 0.6195 - val_binary_accuracy: 0.6878 - val_loss: 0.6055 - learning_rate: 2.5000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6706 - loss: 0.6223 - val_binary_accuracy: 0.6878 - val_loss: 0.6056 - learning_rate: 2.5000e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6926 - loss: 0.6072 - val_binary_accuracy: 0.6878 - val_loss: 0.6061 - learning_rate: 2.5000e-04\n",
      "Epoch 58/5000\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6694 - loss: 0.6239 - val_binary_accuracy: 0.6864 - val_loss: 0.6058 - learning_rate: 2.5000e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6825 - loss: 0.6175 - val_binary_accuracy: 0.6850 - val_loss: 0.6063 - learning_rate: 1.2500e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.6766 - loss: 0.6140 - val_binary_accuracy: 0.6891 - val_loss: 0.6059 - learning_rate: 1.2500e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6827 - loss: 0.6234 - val_binary_accuracy: 0.6891 - val_loss: 0.6060 - learning_rate: 1.2500e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.7010 - loss: 0.6069 - val_binary_accuracy: 0.6891 - val_loss: 0.6057 - learning_rate: 1.2500e-04\n",
      "Epoch 63/5000\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6816 - loss: 0.6122 - val_binary_accuracy: 0.6878 - val_loss: 0.6057 - learning_rate: 1.2500e-04\n",
      "Epoch 64/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.6763 - loss: 0.6192 - val_binary_accuracy: 0.6891 - val_loss: 0.6058 - learning_rate: 6.2500e-05\n",
      "Epoch 65/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6831 - loss: 0.6094 - val_binary_accuracy: 0.6891 - val_loss: 0.6057 - learning_rate: 6.2500e-05\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.6767 - loss: 0.6243 \n",
      "Epoch 1/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6713 - loss: 0.6273 - val_binary_accuracy: 0.6648 - val_loss: 0.6251 - learning_rate: 6.2500e-05\n",
      "Epoch 2/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6703 - loss: 0.6271 - val_binary_accuracy: 0.6676 - val_loss: 0.6246 - learning_rate: 6.2500e-05\n",
      "Epoch 3/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6593 - loss: 0.6334 - val_binary_accuracy: 0.6724 - val_loss: 0.6240 - learning_rate: 6.2500e-05\n",
      "Epoch 4/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6742 - loss: 0.6185 - val_binary_accuracy: 0.6683 - val_loss: 0.6240 - learning_rate: 6.2500e-05\n",
      "Epoch 5/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6683 - loss: 0.6270 - val_binary_accuracy: 0.6676 - val_loss: 0.6237 - learning_rate: 6.2500e-05\n",
      "Epoch 6/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6771 - loss: 0.6209 - val_binary_accuracy: 0.6724 - val_loss: 0.6233 - learning_rate: 6.2500e-05\n",
      "Epoch 7/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6723 - loss: 0.6264 - val_binary_accuracy: 0.6731 - val_loss: 0.6232 - learning_rate: 6.2500e-05\n",
      "Epoch 8/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6743 - loss: 0.6230 - val_binary_accuracy: 0.6738 - val_loss: 0.6229 - learning_rate: 6.2500e-05\n",
      "Epoch 9/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6649 - loss: 0.6289 - val_binary_accuracy: 0.6731 - val_loss: 0.6229 - learning_rate: 6.2500e-05\n",
      "Epoch 10/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6800 - loss: 0.6179 - val_binary_accuracy: 0.6731 - val_loss: 0.6227 - learning_rate: 6.2500e-05\n",
      "Epoch 11/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6732 - loss: 0.6237 - val_binary_accuracy: 0.6738 - val_loss: 0.6225 - learning_rate: 6.2500e-05\n",
      "Epoch 12/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6783 - loss: 0.6178 - val_binary_accuracy: 0.6724 - val_loss: 0.6224 - learning_rate: 6.2500e-05\n",
      "Epoch 13/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6815 - loss: 0.6213 - val_binary_accuracy: 0.6731 - val_loss: 0.6221 - learning_rate: 6.2500e-05\n",
      "Epoch 14/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6649 - loss: 0.6250 - val_binary_accuracy: 0.6724 - val_loss: 0.6222 - learning_rate: 6.2500e-05\n",
      "Epoch 15/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6712 - loss: 0.6242 - val_binary_accuracy: 0.6731 - val_loss: 0.6219 - learning_rate: 6.2500e-05\n",
      "Epoch 16/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6592 - loss: 0.6323 - val_binary_accuracy: 0.6745 - val_loss: 0.6217 - learning_rate: 6.2500e-05\n",
      "Epoch 17/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6650 - loss: 0.6336 - val_binary_accuracy: 0.6717 - val_loss: 0.6217 - learning_rate: 6.2500e-05\n",
      "Epoch 18/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6631 - loss: 0.6243 - val_binary_accuracy: 0.6745 - val_loss: 0.6214 - learning_rate: 6.2500e-05\n",
      "Epoch 19/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6710 - loss: 0.6248 - val_binary_accuracy: 0.6703 - val_loss: 0.6216 - learning_rate: 6.2500e-05\n",
      "Epoch 20/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6668 - loss: 0.6241 - val_binary_accuracy: 0.6710 - val_loss: 0.6214 - learning_rate: 6.2500e-05\n",
      "Epoch 21/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6720 - loss: 0.6244 - val_binary_accuracy: 0.6710 - val_loss: 0.6215 - learning_rate: 6.2500e-05\n",
      "Epoch 22/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6769 - loss: 0.6227 - val_binary_accuracy: 0.6745 - val_loss: 0.6212 - learning_rate: 6.2500e-05\n",
      "Epoch 23/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6765 - loss: 0.6217 - val_binary_accuracy: 0.6752 - val_loss: 0.6211 - learning_rate: 6.2500e-05\n",
      "Epoch 24/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6807 - loss: 0.6223 - val_binary_accuracy: 0.6752 - val_loss: 0.6210 - learning_rate: 6.2500e-05\n",
      "Epoch 25/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6721 - loss: 0.6228 - val_binary_accuracy: 0.6731 - val_loss: 0.6212 - learning_rate: 6.2500e-05\n",
      "Epoch 26/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6811 - loss: 0.6175 - val_binary_accuracy: 0.6738 - val_loss: 0.6210 - learning_rate: 6.2500e-05\n",
      "Epoch 27/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6729 - loss: 0.6202 - val_binary_accuracy: 0.6731 - val_loss: 0.6209 - learning_rate: 6.2500e-05\n",
      "Epoch 28/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6696 - loss: 0.6238 - val_binary_accuracy: 0.6731 - val_loss: 0.6212 - learning_rate: 6.2500e-05\n",
      "Epoch 29/5000\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6681 - loss: 0.6255 - val_binary_accuracy: 0.6731 - val_loss: 0.6210 - learning_rate: 6.2500e-05\n",
      "Epoch 30/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6766 - loss: 0.6168 - val_binary_accuracy: 0.6738 - val_loss: 0.6210 - learning_rate: 3.1250e-05\n",
      "Epoch 31/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6774 - loss: 0.6152 - val_binary_accuracy: 0.6738 - val_loss: 0.6209 - learning_rate: 3.1250e-05\n",
      "Epoch 32/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6719 - loss: 0.6227 - val_binary_accuracy: 0.6731 - val_loss: 0.6210 - learning_rate: 3.1250e-05\n",
      "Epoch 33/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6856 - loss: 0.6187 - val_binary_accuracy: 0.6738 - val_loss: 0.6209 - learning_rate: 3.1250e-05\n",
      "Epoch 34/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6682 - loss: 0.6157 - val_binary_accuracy: 0.6738 - val_loss: 0.6208 - learning_rate: 3.1250e-05\n",
      "Epoch 35/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6655 - loss: 0.6267 - val_binary_accuracy: 0.6717 - val_loss: 0.6209 - learning_rate: 3.1250e-05\n",
      "Epoch 36/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6736 - loss: 0.6197 - val_binary_accuracy: 0.6724 - val_loss: 0.6208 - learning_rate: 3.1250e-05\n",
      "Epoch 37/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6774 - loss: 0.6179 - val_binary_accuracy: 0.6717 - val_loss: 0.6208 - learning_rate: 3.1250e-05\n",
      "Epoch 38/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6764 - loss: 0.6174 - val_binary_accuracy: 0.6717 - val_loss: 0.6209 - learning_rate: 3.1250e-05\n",
      "Epoch 39/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6700 - loss: 0.6143 - val_binary_accuracy: 0.6724 - val_loss: 0.6207 - learning_rate: 3.1250e-05\n",
      "Epoch 40/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6738 - loss: 0.6194 - val_binary_accuracy: 0.6731 - val_loss: 0.6207 - learning_rate: 3.1250e-05\n",
      "Epoch 41/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6598 - loss: 0.6252 - val_binary_accuracy: 0.6731 - val_loss: 0.6207 - learning_rate: 3.1250e-05\n",
      "Epoch 42/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6718 - loss: 0.6189 - val_binary_accuracy: 0.6717 - val_loss: 0.6207 - learning_rate: 3.1250e-05\n",
      "Epoch 43/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6720 - loss: 0.6218 - val_binary_accuracy: 0.6717 - val_loss: 0.6207 - learning_rate: 3.1250e-05\n",
      "Epoch 44/5000\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6650 - loss: 0.6253 - val_binary_accuracy: 0.6724 - val_loss: 0.6206 - learning_rate: 3.1250e-05\n",
      "Epoch 45/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6799 - loss: 0.6183 - val_binary_accuracy: 0.6717 - val_loss: 0.6206 - learning_rate: 1.5625e-05\n",
      "Epoch 46/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6774 - loss: 0.6143 - val_binary_accuracy: 0.6710 - val_loss: 0.6206 - learning_rate: 1.5625e-05\n",
      "Epoch 47/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6850 - loss: 0.6199 - val_binary_accuracy: 0.6710 - val_loss: 0.6205 - learning_rate: 1.5625e-05\n",
      "Epoch 48/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6821 - loss: 0.6176 - val_binary_accuracy: 0.6717 - val_loss: 0.6205 - learning_rate: 1.5625e-05\n",
      "Epoch 49/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6861 - loss: 0.6101 - val_binary_accuracy: 0.6717 - val_loss: 0.6205 - learning_rate: 1.5625e-05\n",
      "Epoch 50/5000\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6628 - loss: 0.6252 - val_binary_accuracy: 0.6717 - val_loss: 0.6206 - learning_rate: 1.5625e-05\n",
      "Epoch 51/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6669 - loss: 0.6197 - val_binary_accuracy: 0.6710 - val_loss: 0.6206 - learning_rate: 7.8125e-06\n",
      "Epoch 52/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6700 - loss: 0.6242 - val_binary_accuracy: 0.6710 - val_loss: 0.6206 - learning_rate: 7.8125e-06\n",
      "Epoch 53/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6862 - loss: 0.6119 - val_binary_accuracy: 0.6710 - val_loss: 0.6205 - learning_rate: 7.8125e-06\n",
      "Epoch 54/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6617 - loss: 0.6311 - val_binary_accuracy: 0.6710 - val_loss: 0.6206 - learning_rate: 7.8125e-06\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.6572 - loss: 0.6340 \n",
      "Epoch 1/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6718 - loss: 0.6215 - val_binary_accuracy: 0.6301 - val_loss: 0.6645 - learning_rate: 7.8125e-06\n",
      "Epoch 2/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6787 - loss: 0.6205 - val_binary_accuracy: 0.6306 - val_loss: 0.6644 - learning_rate: 7.8125e-06\n",
      "Epoch 3/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6769 - loss: 0.6203 - val_binary_accuracy: 0.6310 - val_loss: 0.6642 - learning_rate: 7.8125e-06\n",
      "Epoch 4/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6746 - loss: 0.6196 - val_binary_accuracy: 0.6315 - val_loss: 0.6642 - learning_rate: 7.8125e-06\n",
      "Epoch 5/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6697 - loss: 0.6275 - val_binary_accuracy: 0.6319 - val_loss: 0.6639 - learning_rate: 7.8125e-06\n",
      "Epoch 6/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6758 - loss: 0.6249 - val_binary_accuracy: 0.6315 - val_loss: 0.6641 - learning_rate: 7.8125e-06\n",
      "Epoch 7/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6785 - loss: 0.6226 - val_binary_accuracy: 0.6310 - val_loss: 0.6642 - learning_rate: 7.8125e-06\n",
      "Epoch 8/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6697 - loss: 0.6200 - val_binary_accuracy: 0.6315 - val_loss: 0.6642 - learning_rate: 7.8125e-06\n",
      "Epoch 9/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6709 - loss: 0.6246 - val_binary_accuracy: 0.6306 - val_loss: 0.6638 - learning_rate: 7.8125e-06\n",
      "Epoch 10/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6750 - loss: 0.6206 - val_binary_accuracy: 0.6306 - val_loss: 0.6639 - learning_rate: 7.8125e-06\n",
      "Epoch 11/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6788 - loss: 0.6177 - val_binary_accuracy: 0.6315 - val_loss: 0.6640 - learning_rate: 7.8125e-06\n",
      "Epoch 12/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6708 - loss: 0.6287 - val_binary_accuracy: 0.6301 - val_loss: 0.6639 - learning_rate: 7.8125e-06\n",
      "Epoch 13/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6687 - loss: 0.6235 - val_binary_accuracy: 0.6306 - val_loss: 0.6637 - learning_rate: 7.8125e-06\n",
      "Epoch 14/5000\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6793 - loss: 0.6175 - val_binary_accuracy: 0.6301 - val_loss: 0.6640 - learning_rate: 7.8125e-06\n",
      "Epoch 15/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6720 - loss: 0.6263 - val_binary_accuracy: 0.6301 - val_loss: 0.6639 - learning_rate: 3.9063e-06\n",
      "Epoch 16/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6743 - loss: 0.6191 - val_binary_accuracy: 0.6287 - val_loss: 0.6641 - learning_rate: 3.9063e-06\n",
      "Epoch 17/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6753 - loss: 0.6214 - val_binary_accuracy: 0.6292 - val_loss: 0.6641 - learning_rate: 3.9063e-06\n",
      "Epoch 18/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6661 - loss: 0.6318 - val_binary_accuracy: 0.6292 - val_loss: 0.6642 - learning_rate: 3.9063e-06\n",
      "Epoch 19/5000\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6691 - loss: 0.6253 - val_binary_accuracy: 0.6296 - val_loss: 0.6641 - learning_rate: 3.9063e-06\n",
      "Epoch 20/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6762 - loss: 0.6188 - val_binary_accuracy: 0.6296 - val_loss: 0.6641 - learning_rate: 1.9531e-06\n",
      "Epoch 21/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6692 - loss: 0.6271 - val_binary_accuracy: 0.6292 - val_loss: 0.6640 - learning_rate: 1.9531e-06\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.6609 - loss: 0.6400 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6626273989677429, 0.6405948996543884, 0.6524373292922974],\n",
       " [[array([   0,    1,    2, ..., 3629, 3630, 3631]),\n",
       "   array([3632, 3633, 3634, ..., 7260, 7261, 7262])],\n",
       "  [array([   0,    1,    2, ..., 7260, 7261, 7262]),\n",
       "   array([ 7263,  7264,  7265, ..., 10891, 10892, 10893])],\n",
       "  [array([    0,     1,     2, ..., 10891, 10892, 10893]),\n",
       "   array([10894, 10895, 10896, ..., 14522, 14523, 14524])]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural network with time series cross-validation!\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#load the data, use the maximum number of columns, so don't drop anything\n",
    "base_dir = Path.cwd()\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games = games.drop(columns = ['matchup_home'])\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
    "#drop a column to prevent data leakage\n",
    "games = games.drop(columns = ['plus_minus_home'])\n",
    "games['elo_diff'] = games['elo_home'] - games['elo_away']\n",
    "#games = games.drop(columns = ['elo_home', 'elo_away'])\n",
    "games['rating_diff'] = games['rating_home'] - games['rating_away']\n",
    "#games = games.drop(columns = ['rating_home', 'rating_away'])\n",
    "games = games.drop(columns = ['game_id'])\n",
    "for i in list(range(1,6)):\n",
    "    games['rating_player_diff' + str(i)] = games['rating_home_player' + str(i)] - games['rating_away_player' + str(i)]\n",
    "    #games = games.drop(columns = ['rating_home_player' + str(i), 'rating_away_player' + str(i)])\n",
    "games['home_netrtg'] = games['ortg_home'] - games['drtg_home']\n",
    "#games = games.drop(columns = ['ortg_home', 'drtg_home'])\n",
    "games['away_netrtg'] = games['ortg_away'] - games['drtg_away']\n",
    "#games = games.drop(columns = ['ortg_away', 'drtg_away'])\n",
    "games['home_netrtg_last_season'] = games['home_ortg_last_season'] - games['home_drtg_last_season']\n",
    "games['away_netrtg_last_season'] = games['away_ortg_last_season'] - games['away_drtg_last_season']\n",
    "#games = games.drop(columns = ['home_ortg_last_season', 'home_drtg_last_season', 'away_ortg_last_season', 'away_drtg_last_season'])\n",
    "games['GP_home'] = games['record_home_wins'] + games['record_home_losses']\n",
    "games['GP_away'] = games['record_away_wins'] + games['record_away_losses']\n",
    "#games = games.drop(columns = ['record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses'])\n",
    "games['wl%_diff'] = games['home_wl%'] - games['away_wl%']\n",
    "#games = games.drop(columns = ['home_wl%', 'away_wl%'])\n",
    "games['netrg_diff'] = games['home_netrtg'] - games['away_netrtg']\n",
    "#games = games.drop(columns = ['home_netrtg', 'away_netrtg'])\n",
    "games['netrtg_diff_last_season'] = games['home_netrtg_last_season'] - games['away_netrtg_last_season']\n",
    "#games = games.drop(columns = ['home_netrtg_last_season', 'away_netrtg_last_season'])\n",
    "games.replace('Charlotte Bobcats', 'Charlotte Hornets', inplace=True)\n",
    "games.replace('New Orleans Hornets', 'New Orleans Pelicans', inplace=True)\n",
    "\n",
    "\n",
    "#preprocessing - turning dates into days since first game in table and one-hot encoding team names\n",
    "#now we also rescale the data since this is necessary for neural networks!\n",
    "games['game_date'] = (games['game_date'] - games['game_date'].min()) / pd.Timedelta(days=1)\n",
    "categorical_columns = ['team_name_home', 'team_name_away', 'season_type']\n",
    "numerical_columns = [col for col in data_train.columns if col not in categorical_columns]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('team_names', OneHotEncoder(), categorical_columns),  # Apply OneHotEncoder to team columns\n",
    "        ('numerical_for_rescaling', StandardScaler(), numerical_columns),\n",
    "    ],\n",
    ")\n",
    "\n",
    "#add early stopping and learning rate scheduler to reduce overfitting!\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "#define and compile the neural network\n",
    "model_nn = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu',input_shape=[data_train_processed.shape[1]], kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "model_nn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "indices = []\n",
    "accuracies = []\n",
    "for train_index, test_index in tscv.split(games):\n",
    "    indices.append([train_index, test_index])\n",
    "    data_train = games.iloc[train_index]\n",
    "    target_train = data_train['wl_home']\n",
    "    data_train = data_train.drop(columns = ['wl_home'])\n",
    "    data_test = games.iloc[test_index]\n",
    "    target_test = data_test['wl_home']\n",
    "    data_test = data_test.drop(columns = ['wl_home'])\n",
    "    data_train_processed = preprocessor.fit_transform(data_train)\n",
    "    data_test_processed = preprocessor.transform(data_test)\n",
    "    #finally, train and test the neural network\n",
    "    history = model_nn.fit(data_train_processed, target_train,\n",
    "                           validation_split=0.2,  # Use a portion of training data for validation\n",
    "                           epochs=5000,            # Adjust epochs as needed\n",
    "                           batch_size=256,        # Mini-batch size\n",
    "                           callbacks = [early_stopping, lr_scheduler],\n",
    "                           verbose=1)            # Set verbose to 1 to see the training process\n",
    "    test_loss, test_accuracy = model_nn.evaluate(data_test_processed, target_test)\n",
    "    accuracies.append(test_accuracy)\n",
    "accuracies, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "def99303-f55d-4396-950a-fc4aa8773dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6428\\4251468166.py:21: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices:[[array([   0,    1,    2, ..., 3629, 3630, 3631]), array([3632, 3633, 3634, ..., 7260, 7261, 7262])], [array([   0,    1,    2, ..., 7260, 7261, 7262]), array([ 7263,  7264,  7265, ..., 10891, 10892, 10893])], [array([    0,     1,     2, ..., 10891, 10892, 10893]), array([10894, 10895, 10896, ..., 14522, 14523, 14524])]]\n",
      "With RF: [0.6582208757917929, 0.621041035527403, 0.6505095015147342]\n",
      "With XGB: [0.6527127513081795, 0.6240705039933903, 0.646653814376205]\n"
     ]
    }
   ],
   "source": [
    "# Now I want to apply PCA!\n",
    "\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "#load the data\n",
    "base_dir = Path.cwd()\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games = games.drop(columns = ['matchup_home'])\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
    "#drop a column to prevent data leakage\n",
    "games = games.drop(columns = ['plus_minus_home'])\n",
    "#add the difference in Elo ratings as a column, drop the individual Elo ratings\n",
    "games['elo_diff'] = games['elo_home'] - games['elo_away']\n",
    "games = games.drop(columns = ['elo_home', 'elo_away'])\n",
    "#add the gamescore-offrtg-defrtg rating difference as a column, drop the individual team ratings\n",
    "games['rating_diff'] = games['rating_home'] - games['rating_away']\n",
    "games = games.drop(columns = ['rating_home', 'rating_away'])\n",
    "#drop game_id, it's just an index column\n",
    "games = games.drop(columns = ['game_id'])\n",
    "#add the rating *difference* between the nth best players, but don't drop the individual ratings\n",
    "for i in list(range(1,6)):\n",
    "    games['rating_player_diff' + str(i)] = games['rating_home_player' + str(i)] - games['rating_away_player' + str(i)]\n",
    "    #games = games.drop(columns = ['rating_home_player' + str(i), 'rating_away_player' + str(i)])\n",
    "#add home net rating instead of having both offrtg and defrtg\n",
    "games['home_netrtg'] = games['ortg_home'] - games['drtg_home']\n",
    "games = games.drop(columns = ['ortg_home', 'drtg_home'])\n",
    "#add away net rating...\n",
    "games['away_netrtg'] = games['ortg_away'] - games['drtg_away']\n",
    "games = games.drop(columns = ['ortg_away', 'drtg_away'])\n",
    "#add home net rating and away net rating from last season\n",
    "games['home_netrtg_last_season'] = games['home_ortg_last_season'] - games['home_drtg_last_season']\n",
    "games['away_netrtg_last_season'] = games['away_ortg_last_season'] - games['away_drtg_last_season']\n",
    "games = games.drop(columns = ['home_ortg_last_season', 'home_drtg_last_season', 'away_ortg_last_season', 'away_drtg_last_season'])\n",
    "#add games played home/away (games played so far this season)\n",
    "games['GP_home'] = games['record_home_wins'] + games['record_home_losses']\n",
    "games['GP_away'] = games['record_away_wins'] + games['record_away_losses']\n",
    "games = games.drop(columns = ['record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses'])\n",
    "#add difference in win-loss percentage, remove individual win-loss percentages\n",
    "games['wl%_diff'] = games['home_wl%'] - games['away_wl%']\n",
    "games = games.drop(columns = ['home_wl%', 'away_wl%'])\n",
    "#add net rating difference, but keep individual home/away net ratings - apparently the model works better with them kept in\n",
    "games['netrg_diff'] = games['home_netrtg'] - games['away_netrtg']\n",
    "#games = games.drop(columns = ['home_netrtg', 'away_netrtg'])\n",
    "#add netrtg difference from last season, drop individual net rtgs\n",
    "games['netrtg_diff_last_season'] = games['home_netrtg_last_season'] - games['away_netrtg_last_season']\n",
    "games = games.drop(columns = ['home_netrtg_last_season', 'away_netrtg_last_season'])\n",
    "#in 2013-14, the charlotte hornets were called the bobcats, and (maybe) the new orleans pelicans were called the hornets\n",
    "games.replace('Charlotte Bobcats', 'Charlotte Hornets', inplace=True)\n",
    "games.replace('New Orleans Hornets', 'New Orleans Pelicans', inplace=True)\n",
    "#split into training and testing set\n",
    "# games_train = games[(games['game_date'] <= datetime.datetime(2023,9,30))]\n",
    "# target_train = games_train['wl_home']\n",
    "# data_train = games_train.drop(columns = ['wl_home'])\n",
    "# games_test = games[games['game_date'] >= datetime.datetime(2023,10,1)]\n",
    "# target_test = games_test['wl_home']\n",
    "# data_test = games_test.drop(columns = ['wl_home'])\n",
    "\n",
    "#preprocessing - turning dates into days since first game in table and one-hot encoding team names\n",
    "games['game_date'] = (games['game_date'] - games['game_date'].min()) / pd.Timedelta(days=1)\n",
    "wl_home = games['wl_home']\n",
    "games = games.drop(columns = ['wl_home'])\n",
    "categorical_columns = ['team_name_home', 'team_name_away', 'season_type']\n",
    "numerical_columns = [col for col in games.columns if col not in categorical_columns]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('team_names', OneHotEncoder(), categorical_columns),  # Apply OneHotEncoder to team columns\n",
    "        ('standard_scale_num_cols', StandardScaler(), numerical_columns) # Scale numerical columns for PCA\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "#fit and test the model - the n_estimators was tuned - other parameters gave the best results with the default values\n",
    "accuracies_rf = []\n",
    "accuracies_xgb = []\n",
    "indices = []\n",
    "classifier = RandomForestClassifier(n_estimators = 400, n_jobs = 2, random_state = 0)\n",
    "pca = PCA(n_components=30)  # Reduce to 30 dimensions\n",
    "model = make_pipeline(preprocessor, pca, classifier)\n",
    "preprocessor_xgb = make_pipeline(preprocessor, pca)\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_index, test_index in tscv.split(games):\n",
    "    indices.append([train_index, test_index])\n",
    "    data_train = games.iloc[train_index]\n",
    "    data_test = games.iloc[test_index]\n",
    "    target_train = wl_home[train_index]\n",
    "    target_test = wl_home[test_index]\n",
    "    model.fit(data_train, target_train)\n",
    "    accuracies_rf.append(model.score(data_test, target_test))\n",
    "    #now use xgboost on the same data...\n",
    "    data_train_processed = preprocessor_xgb.fit_transform(data_train)\n",
    "    data_test_processed = preprocessor_xgb.transform(data_test)\n",
    "    model_xgb = XGBClassifier(random_state=0, learning_rate=0.01, n_estimators=200)\n",
    "    model_xgb.fit(data_train_processed, target_train)\n",
    "    accuracies_xgb.append(model_xgb.score(data_test_processed, target_test))\n",
    "print(\"Indices:\" + str(indices))\n",
    "print(\"With RF: \" + str(accuracies_rf))\n",
    "print(\"With XGB: \" + str(accuracies_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "081b1337-b6c1-409a-96f4-167fd5eb914d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6428\\2116242924.py:21: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - binary_accuracy: 0.5834 - loss: 3.6660 - val_binary_accuracy: 0.6919 - val_loss: 3.1907 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6424 - loss: 3.1133 - val_binary_accuracy: 0.6905 - val_loss: 2.7410 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6634 - loss: 2.6722 - val_binary_accuracy: 0.6864 - val_loss: 2.3524 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6562 - loss: 2.2936 - val_binary_accuracy: 0.6809 - val_loss: 2.0206 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6590 - loss: 1.9634 - val_binary_accuracy: 0.6699 - val_loss: 1.7418 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6858 - loss: 1.6948 - val_binary_accuracy: 0.6809 - val_loss: 1.5149 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6593 - loss: 1.4899 - val_binary_accuracy: 0.6795 - val_loss: 1.3304 - learning_rate: 0.0010\n",
      "Epoch 8/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6734 - loss: 1.3062 - val_binary_accuracy: 0.6878 - val_loss: 1.1793 - learning_rate: 0.0010\n",
      "Epoch 9/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6593 - loss: 1.1705 - val_binary_accuracy: 0.6864 - val_loss: 1.0615 - learning_rate: 0.0010\n",
      "Epoch 10/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6679 - loss: 1.0646 - val_binary_accuracy: 0.6919 - val_loss: 0.9653 - learning_rate: 0.0010\n",
      "Epoch 11/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6692 - loss: 0.9657 - val_binary_accuracy: 0.6974 - val_loss: 0.8893 - learning_rate: 0.0010\n",
      "Epoch 12/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6839 - loss: 0.8893 - val_binary_accuracy: 0.6933 - val_loss: 0.8292 - learning_rate: 0.0010\n",
      "Epoch 13/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6835 - loss: 0.8366 - val_binary_accuracy: 0.6919 - val_loss: 0.7825 - learning_rate: 0.0010\n",
      "Epoch 14/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6744 - loss: 0.7934 - val_binary_accuracy: 0.6974 - val_loss: 0.7460 - learning_rate: 0.0010\n",
      "Epoch 15/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6846 - loss: 0.7470 - val_binary_accuracy: 0.6905 - val_loss: 0.7162 - learning_rate: 0.0010\n",
      "Epoch 16/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6752 - loss: 0.7311 - val_binary_accuracy: 0.6905 - val_loss: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 17/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6755 - loss: 0.7009 - val_binary_accuracy: 0.6960 - val_loss: 0.6747 - learning_rate: 0.0010\n",
      "Epoch 18/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6770 - loss: 0.6807 - val_binary_accuracy: 0.6891 - val_loss: 0.6606 - learning_rate: 0.0010\n",
      "Epoch 19/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6828 - loss: 0.6665 - val_binary_accuracy: 0.6905 - val_loss: 0.6503 - learning_rate: 0.0010\n",
      "Epoch 20/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6890 - loss: 0.6594 - val_binary_accuracy: 0.6905 - val_loss: 0.6425 - learning_rate: 0.0010\n",
      "Epoch 21/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6756 - loss: 0.6564 - val_binary_accuracy: 0.6919 - val_loss: 0.6348 - learning_rate: 0.0010\n",
      "Epoch 22/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6640 - loss: 0.6573 - val_binary_accuracy: 0.6891 - val_loss: 0.6290 - learning_rate: 0.0010\n",
      "Epoch 23/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6661 - loss: 0.6497 - val_binary_accuracy: 0.6864 - val_loss: 0.6240 - learning_rate: 0.0010\n",
      "Epoch 24/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6736 - loss: 0.6376 - val_binary_accuracy: 0.6891 - val_loss: 0.6201 - learning_rate: 0.0010\n",
      "Epoch 25/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6754 - loss: 0.6319 - val_binary_accuracy: 0.6891 - val_loss: 0.6180 - learning_rate: 0.0010\n",
      "Epoch 26/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6827 - loss: 0.6297 - val_binary_accuracy: 0.6891 - val_loss: 0.6161 - learning_rate: 0.0010\n",
      "Epoch 27/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6725 - loss: 0.6355 - val_binary_accuracy: 0.6905 - val_loss: 0.6145 - learning_rate: 0.0010\n",
      "Epoch 28/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6718 - loss: 0.6249 - val_binary_accuracy: 0.6905 - val_loss: 0.6133 - learning_rate: 0.0010\n",
      "Epoch 29/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6809 - loss: 0.6267 - val_binary_accuracy: 0.6905 - val_loss: 0.6117 - learning_rate: 0.0010\n",
      "Epoch 30/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6914 - loss: 0.6163 - val_binary_accuracy: 0.6878 - val_loss: 0.6096 - learning_rate: 0.0010\n",
      "Epoch 31/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6837 - loss: 0.6213 - val_binary_accuracy: 0.6836 - val_loss: 0.6098 - learning_rate: 0.0010\n",
      "Epoch 32/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6681 - loss: 0.6256 - val_binary_accuracy: 0.6850 - val_loss: 0.6081 - learning_rate: 0.0010\n",
      "Epoch 33/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6746 - loss: 0.6294 - val_binary_accuracy: 0.6905 - val_loss: 0.6086 - learning_rate: 0.0010\n",
      "Epoch 34/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6721 - loss: 0.6203 - val_binary_accuracy: 0.6891 - val_loss: 0.6072 - learning_rate: 0.0010\n",
      "Epoch 35/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6844 - loss: 0.6117 - val_binary_accuracy: 0.6878 - val_loss: 0.6090 - learning_rate: 0.0010\n",
      "Epoch 36/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6842 - loss: 0.6167 - val_binary_accuracy: 0.6878 - val_loss: 0.6071 - learning_rate: 0.0010\n",
      "Epoch 37/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6773 - loss: 0.6187 - val_binary_accuracy: 0.6891 - val_loss: 0.6065 - learning_rate: 0.0010\n",
      "Epoch 38/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6766 - loss: 0.6262 - val_binary_accuracy: 0.6933 - val_loss: 0.6077 - learning_rate: 0.0010\n",
      "Epoch 39/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6730 - loss: 0.6225 - val_binary_accuracy: 0.6891 - val_loss: 0.6072 - learning_rate: 0.0010\n",
      "Epoch 40/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6849 - loss: 0.6158 - val_binary_accuracy: 0.6878 - val_loss: 0.6068 - learning_rate: 0.0010\n",
      "Epoch 41/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6750 - loss: 0.6168 - val_binary_accuracy: 0.6891 - val_loss: 0.6067 - learning_rate: 0.0010\n",
      "Epoch 42/5000\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6850 - loss: 0.6175 - val_binary_accuracy: 0.6891 - val_loss: 0.6086 - learning_rate: 0.0010\n",
      "Epoch 43/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6863 - loss: 0.6177 - val_binary_accuracy: 0.6905 - val_loss: 0.6076 - learning_rate: 5.0000e-04\n",
      "Epoch 44/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6806 - loss: 0.6141 - val_binary_accuracy: 0.6905 - val_loss: 0.6063 - learning_rate: 5.0000e-04\n",
      "Epoch 45/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6723 - loss: 0.6170 - val_binary_accuracy: 0.6878 - val_loss: 0.6060 - learning_rate: 5.0000e-04\n",
      "Epoch 46/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6932 - loss: 0.6151 - val_binary_accuracy: 0.6891 - val_loss: 0.6059 - learning_rate: 5.0000e-04\n",
      "Epoch 47/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6712 - loss: 0.6294 - val_binary_accuracy: 0.6850 - val_loss: 0.6054 - learning_rate: 5.0000e-04\n",
      "Epoch 48/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6871 - loss: 0.6008 - val_binary_accuracy: 0.6878 - val_loss: 0.6057 - learning_rate: 5.0000e-04\n",
      "Epoch 49/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6752 - loss: 0.6153 - val_binary_accuracy: 0.6850 - val_loss: 0.6068 - learning_rate: 5.0000e-04\n",
      "Epoch 50/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6877 - loss: 0.6087 - val_binary_accuracy: 0.6878 - val_loss: 0.6065 - learning_rate: 5.0000e-04\n",
      "Epoch 51/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6799 - loss: 0.6126 - val_binary_accuracy: 0.6864 - val_loss: 0.6079 - learning_rate: 5.0000e-04\n",
      "Epoch 52/5000\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6984 - loss: 0.6046 - val_binary_accuracy: 0.6891 - val_loss: 0.6065 - learning_rate: 5.0000e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.7046 - loss: 0.6045 - val_binary_accuracy: 0.6891 - val_loss: 0.6069 - learning_rate: 2.5000e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6846 - loss: 0.6125 - val_binary_accuracy: 0.6878 - val_loss: 0.6066 - learning_rate: 2.5000e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6729 - loss: 0.6119 - val_binary_accuracy: 0.6864 - val_loss: 0.6058 - learning_rate: 2.5000e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6751 - loss: 0.6138 - val_binary_accuracy: 0.6891 - val_loss: 0.6063 - learning_rate: 2.5000e-04\n",
      "Epoch 57/5000\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6843 - loss: 0.6155 - val_binary_accuracy: 0.6891 - val_loss: 0.6057 - learning_rate: 2.5000e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6845 - loss: 0.6214 - val_binary_accuracy: 0.6891 - val_loss: 0.6059 - learning_rate: 1.2500e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6800 - loss: 0.6155 - val_binary_accuracy: 0.6891 - val_loss: 0.6056 - learning_rate: 1.2500e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6897 - loss: 0.6086 - val_binary_accuracy: 0.6905 - val_loss: 0.6058 - learning_rate: 1.2500e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - binary_accuracy: 0.6946 - loss: 0.6075 - val_binary_accuracy: 0.6919 - val_loss: 0.6064 - learning_rate: 1.2500e-04\n",
      "Epoch 62/5000\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6895 - loss: 0.6106 - val_binary_accuracy: 0.6878 - val_loss: 0.6059 - learning_rate: 1.2500e-04\n",
      "Epoch 63/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6918 - loss: 0.6120 - val_binary_accuracy: 0.6891 - val_loss: 0.6058 - learning_rate: 6.2500e-05\n",
      "Epoch 64/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6931 - loss: 0.6034 - val_binary_accuracy: 0.6891 - val_loss: 0.6058 - learning_rate: 6.2500e-05\n",
      "Epoch 65/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6774 - loss: 0.6142 - val_binary_accuracy: 0.6891 - val_loss: 0.6060 - learning_rate: 6.2500e-05\n",
      "Epoch 66/5000\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - binary_accuracy: 0.6710 - loss: 0.6214 - val_binary_accuracy: 0.6891 - val_loss: 0.6061 - learning_rate: 6.2500e-05\n",
      "Epoch 67/5000\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - binary_accuracy: 0.6822 - loss: 0.6176 - val_binary_accuracy: 0.6891 - val_loss: 0.6058 - learning_rate: 6.2500e-05\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.6781 - loss: 0.6221 \n",
      "Epoch 1/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6676 - loss: 0.6230 - val_binary_accuracy: 0.6703 - val_loss: 0.6271 - learning_rate: 3.1250e-05\n",
      "Epoch 2/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6638 - loss: 0.6289 - val_binary_accuracy: 0.6703 - val_loss: 0.6267 - learning_rate: 3.1250e-05\n",
      "Epoch 3/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6790 - loss: 0.6244 - val_binary_accuracy: 0.6703 - val_loss: 0.6266 - learning_rate: 3.1250e-05\n",
      "Epoch 4/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6690 - loss: 0.6321 - val_binary_accuracy: 0.6696 - val_loss: 0.6262 - learning_rate: 3.1250e-05\n",
      "Epoch 5/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6630 - loss: 0.6315 - val_binary_accuracy: 0.6710 - val_loss: 0.6259 - learning_rate: 3.1250e-05\n",
      "Epoch 6/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6712 - loss: 0.6276 - val_binary_accuracy: 0.6703 - val_loss: 0.6257 - learning_rate: 3.1250e-05\n",
      "Epoch 7/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6647 - loss: 0.6310 - val_binary_accuracy: 0.6717 - val_loss: 0.6255 - learning_rate: 3.1250e-05\n",
      "Epoch 8/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6679 - loss: 0.6336 - val_binary_accuracy: 0.6731 - val_loss: 0.6253 - learning_rate: 3.1250e-05\n",
      "Epoch 9/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6669 - loss: 0.6267 - val_binary_accuracy: 0.6738 - val_loss: 0.6249 - learning_rate: 3.1250e-05\n",
      "Epoch 10/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6688 - loss: 0.6215 - val_binary_accuracy: 0.6731 - val_loss: 0.6247 - learning_rate: 3.1250e-05\n",
      "Epoch 11/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6638 - loss: 0.6272 - val_binary_accuracy: 0.6731 - val_loss: 0.6245 - learning_rate: 3.1250e-05\n",
      "Epoch 12/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6708 - loss: 0.6277 - val_binary_accuracy: 0.6724 - val_loss: 0.6245 - learning_rate: 3.1250e-05\n",
      "Epoch 13/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6750 - loss: 0.6204 - val_binary_accuracy: 0.6731 - val_loss: 0.6241 - learning_rate: 3.1250e-05\n",
      "Epoch 14/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6772 - loss: 0.6204 - val_binary_accuracy: 0.6724 - val_loss: 0.6239 - learning_rate: 3.1250e-05\n",
      "Epoch 15/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6671 - loss: 0.6211 - val_binary_accuracy: 0.6724 - val_loss: 0.6238 - learning_rate: 3.1250e-05\n",
      "Epoch 16/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6662 - loss: 0.6236 - val_binary_accuracy: 0.6724 - val_loss: 0.6235 - learning_rate: 3.1250e-05\n",
      "Epoch 17/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6754 - loss: 0.6219 - val_binary_accuracy: 0.6724 - val_loss: 0.6234 - learning_rate: 3.1250e-05\n",
      "Epoch 18/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6665 - loss: 0.6262 - val_binary_accuracy: 0.6724 - val_loss: 0.6233 - learning_rate: 3.1250e-05\n",
      "Epoch 19/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6658 - loss: 0.6249 - val_binary_accuracy: 0.6731 - val_loss: 0.6230 - learning_rate: 3.1250e-05\n",
      "Epoch 20/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6781 - loss: 0.6154 - val_binary_accuracy: 0.6731 - val_loss: 0.6229 - learning_rate: 3.1250e-05\n",
      "Epoch 21/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6757 - loss: 0.6223 - val_binary_accuracy: 0.6738 - val_loss: 0.6227 - learning_rate: 3.1250e-05\n",
      "Epoch 22/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6697 - loss: 0.6262 - val_binary_accuracy: 0.6738 - val_loss: 0.6227 - learning_rate: 3.1250e-05\n",
      "Epoch 23/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6751 - loss: 0.6201 - val_binary_accuracy: 0.6745 - val_loss: 0.6226 - learning_rate: 3.1250e-05\n",
      "Epoch 24/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6767 - loss: 0.6215 - val_binary_accuracy: 0.6731 - val_loss: 0.6224 - learning_rate: 3.1250e-05\n",
      "Epoch 25/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6797 - loss: 0.6182 - val_binary_accuracy: 0.6717 - val_loss: 0.6222 - learning_rate: 3.1250e-05\n",
      "Epoch 26/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6591 - loss: 0.6265 - val_binary_accuracy: 0.6717 - val_loss: 0.6220 - learning_rate: 3.1250e-05\n",
      "Epoch 27/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6703 - loss: 0.6183 - val_binary_accuracy: 0.6724 - val_loss: 0.6220 - learning_rate: 3.1250e-05\n",
      "Epoch 28/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6814 - loss: 0.6172 - val_binary_accuracy: 0.6724 - val_loss: 0.6219 - learning_rate: 3.1250e-05\n",
      "Epoch 29/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6760 - loss: 0.6187 - val_binary_accuracy: 0.6724 - val_loss: 0.6219 - learning_rate: 3.1250e-05\n",
      "Epoch 30/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6699 - loss: 0.6221 - val_binary_accuracy: 0.6724 - val_loss: 0.6218 - learning_rate: 3.1250e-05\n",
      "Epoch 31/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6838 - loss: 0.6175 - val_binary_accuracy: 0.6724 - val_loss: 0.6217 - learning_rate: 3.1250e-05\n",
      "Epoch 32/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6670 - loss: 0.6210 - val_binary_accuracy: 0.6717 - val_loss: 0.6216 - learning_rate: 3.1250e-05\n",
      "Epoch 33/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6782 - loss: 0.6187 - val_binary_accuracy: 0.6717 - val_loss: 0.6216 - learning_rate: 3.1250e-05\n",
      "Epoch 34/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6748 - loss: 0.6195 - val_binary_accuracy: 0.6731 - val_loss: 0.6215 - learning_rate: 3.1250e-05\n",
      "Epoch 35/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6717 - loss: 0.6195 - val_binary_accuracy: 0.6752 - val_loss: 0.6212 - learning_rate: 3.1250e-05\n",
      "Epoch 36/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6697 - loss: 0.6217 - val_binary_accuracy: 0.6745 - val_loss: 0.6213 - learning_rate: 3.1250e-05\n",
      "Epoch 37/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6575 - loss: 0.6281 - val_binary_accuracy: 0.6738 - val_loss: 0.6212 - learning_rate: 3.1250e-05\n",
      "Epoch 38/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6727 - loss: 0.6225 - val_binary_accuracy: 0.6738 - val_loss: 0.6212 - learning_rate: 3.1250e-05\n",
      "Epoch 39/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6729 - loss: 0.6206 - val_binary_accuracy: 0.6745 - val_loss: 0.6211 - learning_rate: 3.1250e-05\n",
      "Epoch 40/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6800 - loss: 0.6201 - val_binary_accuracy: 0.6731 - val_loss: 0.6211 - learning_rate: 3.1250e-05\n",
      "Epoch 41/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6761 - loss: 0.6203 - val_binary_accuracy: 0.6772 - val_loss: 0.6209 - learning_rate: 3.1250e-05\n",
      "Epoch 42/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6666 - loss: 0.6223 - val_binary_accuracy: 0.6765 - val_loss: 0.6209 - learning_rate: 3.1250e-05\n",
      "Epoch 43/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6720 - loss: 0.6187 - val_binary_accuracy: 0.6765 - val_loss: 0.6208 - learning_rate: 3.1250e-05\n",
      "Epoch 44/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6605 - loss: 0.6271 - val_binary_accuracy: 0.6765 - val_loss: 0.6207 - learning_rate: 3.1250e-05\n",
      "Epoch 45/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6796 - loss: 0.6200 - val_binary_accuracy: 0.6772 - val_loss: 0.6207 - learning_rate: 3.1250e-05\n",
      "Epoch 46/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6681 - loss: 0.6198 - val_binary_accuracy: 0.6779 - val_loss: 0.6206 - learning_rate: 3.1250e-05\n",
      "Epoch 47/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6761 - loss: 0.6160 - val_binary_accuracy: 0.6765 - val_loss: 0.6206 - learning_rate: 3.1250e-05\n",
      "Epoch 48/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6881 - loss: 0.6146 - val_binary_accuracy: 0.6765 - val_loss: 0.6204 - learning_rate: 3.1250e-05\n",
      "Epoch 49/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6666 - loss: 0.6211 - val_binary_accuracy: 0.6765 - val_loss: 0.6204 - learning_rate: 3.1250e-05\n",
      "Epoch 50/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6623 - loss: 0.6204 - val_binary_accuracy: 0.6758 - val_loss: 0.6203 - learning_rate: 3.1250e-05\n",
      "Epoch 51/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6685 - loss: 0.6174 - val_binary_accuracy: 0.6765 - val_loss: 0.6205 - learning_rate: 3.1250e-05\n",
      "Epoch 52/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6782 - loss: 0.6135 - val_binary_accuracy: 0.6758 - val_loss: 0.6203 - learning_rate: 3.1250e-05\n",
      "Epoch 53/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6668 - loss: 0.6251 - val_binary_accuracy: 0.6752 - val_loss: 0.6203 - learning_rate: 3.1250e-05\n",
      "Epoch 54/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6780 - loss: 0.6147 - val_binary_accuracy: 0.6765 - val_loss: 0.6203 - learning_rate: 3.1250e-05\n",
      "Epoch 55/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6660 - loss: 0.6251 - val_binary_accuracy: 0.6765 - val_loss: 0.6201 - learning_rate: 3.1250e-05\n",
      "Epoch 56/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6688 - loss: 0.6224 - val_binary_accuracy: 0.6758 - val_loss: 0.6201 - learning_rate: 3.1250e-05\n",
      "Epoch 57/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6729 - loss: 0.6232 - val_binary_accuracy: 0.6772 - val_loss: 0.6200 - learning_rate: 3.1250e-05\n",
      "Epoch 58/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6664 - loss: 0.6290 - val_binary_accuracy: 0.6765 - val_loss: 0.6200 - learning_rate: 3.1250e-05\n",
      "Epoch 59/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6762 - loss: 0.6160 - val_binary_accuracy: 0.6772 - val_loss: 0.6199 - learning_rate: 3.1250e-05\n",
      "Epoch 60/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6763 - loss: 0.6173 - val_binary_accuracy: 0.6779 - val_loss: 0.6200 - learning_rate: 3.1250e-05\n",
      "Epoch 61/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6719 - loss: 0.6165 - val_binary_accuracy: 0.6752 - val_loss: 0.6198 - learning_rate: 3.1250e-05\n",
      "Epoch 62/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6750 - loss: 0.6210 - val_binary_accuracy: 0.6772 - val_loss: 0.6199 - learning_rate: 3.1250e-05\n",
      "Epoch 63/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6672 - loss: 0.6249 - val_binary_accuracy: 0.6758 - val_loss: 0.6199 - learning_rate: 3.1250e-05\n",
      "Epoch 64/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6717 - loss: 0.6244 - val_binary_accuracy: 0.6779 - val_loss: 0.6200 - learning_rate: 3.1250e-05\n",
      "Epoch 65/5000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6786 - loss: 0.6165 - val_binary_accuracy: 0.6772 - val_loss: 0.6199 - learning_rate: 3.1250e-05\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - binary_accuracy: 0.6606 - loss: 0.6311\n",
      "Epoch 1/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6753 - loss: 0.6204 - val_binary_accuracy: 0.6283 - val_loss: 0.6637 - learning_rate: 3.1250e-05\n",
      "Epoch 2/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6594 - loss: 0.6285 - val_binary_accuracy: 0.6287 - val_loss: 0.6632 - learning_rate: 3.1250e-05\n",
      "Epoch 3/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6722 - loss: 0.6258 - val_binary_accuracy: 0.6292 - val_loss: 0.6630 - learning_rate: 3.1250e-05\n",
      "Epoch 4/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6723 - loss: 0.6219 - val_binary_accuracy: 0.6301 - val_loss: 0.6633 - learning_rate: 3.1250e-05\n",
      "Epoch 5/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6661 - loss: 0.6219 - val_binary_accuracy: 0.6324 - val_loss: 0.6632 - learning_rate: 3.1250e-05\n",
      "Epoch 6/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6739 - loss: 0.6243 - val_binary_accuracy: 0.6301 - val_loss: 0.6634 - learning_rate: 3.1250e-05\n",
      "Epoch 7/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6788 - loss: 0.6177 - val_binary_accuracy: 0.6292 - val_loss: 0.6633 - learning_rate: 3.1250e-05\n",
      "Epoch 8/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6686 - loss: 0.6233 - val_binary_accuracy: 0.6324 - val_loss: 0.6629 - learning_rate: 3.1250e-05\n",
      "Epoch 9/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6696 - loss: 0.6244 - val_binary_accuracy: 0.6283 - val_loss: 0.6632 - learning_rate: 3.1250e-05\n",
      "Epoch 10/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.6680 - loss: 0.6194 - val_binary_accuracy: 0.6315 - val_loss: 0.6626 - learning_rate: 3.1250e-05\n",
      "Epoch 11/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6738 - loss: 0.6196 - val_binary_accuracy: 0.6333 - val_loss: 0.6622 - learning_rate: 3.1250e-05\n",
      "Epoch 12/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6766 - loss: 0.6171 - val_binary_accuracy: 0.6310 - val_loss: 0.6627 - learning_rate: 3.1250e-05\n",
      "Epoch 13/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6691 - loss: 0.6245 - val_binary_accuracy: 0.6301 - val_loss: 0.6628 - learning_rate: 3.1250e-05\n",
      "Epoch 14/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6716 - loss: 0.6288 - val_binary_accuracy: 0.6301 - val_loss: 0.6623 - learning_rate: 3.1250e-05\n",
      "Epoch 15/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6712 - loss: 0.6190 - val_binary_accuracy: 0.6306 - val_loss: 0.6625 - learning_rate: 3.1250e-05\n",
      "Epoch 16/5000\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6731 - loss: 0.6179 - val_binary_accuracy: 0.6292 - val_loss: 0.6624 - learning_rate: 3.1250e-05\n",
      "Epoch 17/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6839 - loss: 0.6161 - val_binary_accuracy: 0.6292 - val_loss: 0.6622 - learning_rate: 1.5625e-05\n",
      "Epoch 18/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6704 - loss: 0.6232 - val_binary_accuracy: 0.6296 - val_loss: 0.6625 - learning_rate: 1.5625e-05\n",
      "Epoch 19/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6792 - loss: 0.6173 - val_binary_accuracy: 0.6306 - val_loss: 0.6621 - learning_rate: 1.5625e-05\n",
      "Epoch 20/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6652 - loss: 0.6226 - val_binary_accuracy: 0.6292 - val_loss: 0.6623 - learning_rate: 1.5625e-05\n",
      "Epoch 21/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6677 - loss: 0.6263 - val_binary_accuracy: 0.6315 - val_loss: 0.6624 - learning_rate: 1.5625e-05\n",
      "Epoch 22/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6729 - loss: 0.6227 - val_binary_accuracy: 0.6283 - val_loss: 0.6626 - learning_rate: 1.5625e-05\n",
      "Epoch 23/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6685 - loss: 0.6242 - val_binary_accuracy: 0.6296 - val_loss: 0.6626 - learning_rate: 1.5625e-05\n",
      "Epoch 24/5000\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6704 - loss: 0.6187 - val_binary_accuracy: 0.6292 - val_loss: 0.6627 - learning_rate: 1.5625e-05\n",
      "Epoch 25/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6796 - loss: 0.6135 - val_binary_accuracy: 0.6292 - val_loss: 0.6627 - learning_rate: 7.8125e-06\n",
      "Epoch 26/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6670 - loss: 0.6178 - val_binary_accuracy: 0.6292 - val_loss: 0.6626 - learning_rate: 7.8125e-06\n",
      "Epoch 27/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6759 - loss: 0.6173 - val_binary_accuracy: 0.6287 - val_loss: 0.6627 - learning_rate: 7.8125e-06\n",
      "Epoch 28/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6716 - loss: 0.6225 - val_binary_accuracy: 0.6292 - val_loss: 0.6624 - learning_rate: 7.8125e-06\n",
      "Epoch 29/5000\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6792 - loss: 0.6165 - val_binary_accuracy: 0.6292 - val_loss: 0.6625 - learning_rate: 7.8125e-06\n",
      "Epoch 30/5000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6690 - loss: 0.6180 - val_binary_accuracy: 0.6292 - val_loss: 0.6626 - learning_rate: 3.9063e-06\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.6585 - loss: 0.6369 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6681355237960815, 0.6414210796356201, 0.6513357162475586],\n",
       " [[array([   0,    1,    2, ..., 3629, 3630, 3631]),\n",
       "   array([3632, 3633, 3634, ..., 7260, 7261, 7262])],\n",
       "  [array([   0,    1,    2, ..., 7260, 7261, 7262]),\n",
       "   array([ 7263,  7264,  7265, ..., 10891, 10892, 10893])],\n",
       "  [array([    0,     1,     2, ..., 10891, 10892, 10893]),\n",
       "   array([10894, 10895, 10896, ..., 14522, 14523, 14524])]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we add PCA to the neural network.\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#load the data, use the maximum number of columns, so don't drop anything\n",
    "base_dir = Path.cwd()\n",
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games = games.drop(columns = ['matchup_home'])\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "games['wl_home'] = games['wl_home'].replace({'W': 1, 'L': 0})\n",
    "#drop a column to prevent data leakage\n",
    "games = games.drop(columns = ['plus_minus_home'])\n",
    "games['elo_diff'] = games['elo_home'] - games['elo_away']\n",
    "#games = games.drop(columns = ['elo_home', 'elo_away'])\n",
    "games['rating_diff'] = games['rating_home'] - games['rating_away']\n",
    "#games = games.drop(columns = ['rating_home', 'rating_away'])\n",
    "games = games.drop(columns = ['game_id'])\n",
    "for i in list(range(1,6)):\n",
    "    games['rating_player_diff' + str(i)] = games['rating_home_player' + str(i)] - games['rating_away_player' + str(i)]\n",
    "    #games = games.drop(columns = ['rating_home_player' + str(i), 'rating_away_player' + str(i)])\n",
    "games['home_netrtg'] = games['ortg_home'] - games['drtg_home']\n",
    "#games = games.drop(columns = ['ortg_home', 'drtg_home'])\n",
    "games['away_netrtg'] = games['ortg_away'] - games['drtg_away']\n",
    "#games = games.drop(columns = ['ortg_away', 'drtg_away'])\n",
    "games['home_netrtg_last_season'] = games['home_ortg_last_season'] - games['home_drtg_last_season']\n",
    "games['away_netrtg_last_season'] = games['away_ortg_last_season'] - games['away_drtg_last_season']\n",
    "#games = games.drop(columns = ['home_ortg_last_season', 'home_drtg_last_season', 'away_ortg_last_season', 'away_drtg_last_season'])\n",
    "games['GP_home'] = games['record_home_wins'] + games['record_home_losses']\n",
    "games['GP_away'] = games['record_away_wins'] + games['record_away_losses']\n",
    "#games = games.drop(columns = ['record_home_wins', 'record_home_losses', 'record_away_wins', 'record_away_losses'])\n",
    "games['wl%_diff'] = games['home_wl%'] - games['away_wl%']\n",
    "#games = games.drop(columns = ['home_wl%', 'away_wl%'])\n",
    "games['netrg_diff'] = games['home_netrtg'] - games['away_netrtg']\n",
    "#games = games.drop(columns = ['home_netrtg', 'away_netrtg'])\n",
    "games['netrtg_diff_last_season'] = games['home_netrtg_last_season'] - games['away_netrtg_last_season']\n",
    "#games = games.drop(columns = ['home_netrtg_last_season', 'away_netrtg_last_season'])\n",
    "games.replace('Charlotte Bobcats', 'Charlotte Hornets', inplace=True)\n",
    "games.replace('New Orleans Hornets', 'New Orleans Pelicans', inplace=True)\n",
    "\n",
    "\n",
    "#preprocessing - turning dates into days since first game in table and one-hot encoding team names\n",
    "#now we also rescale the data since this is necessary for neural networks!\n",
    "games['game_date'] = (games['game_date'] - games['game_date'].min()) / pd.Timedelta(days=1)\n",
    "wl_home = games['wl_home']\n",
    "games = games.drop(columns = ['wl_home'])\n",
    "categorical_columns = ['team_name_home', 'team_name_away', 'season_type']\n",
    "numerical_columns = [col for col in data_train.columns if col not in categorical_columns]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('team_names', OneHotEncoder(), categorical_columns),  # Apply OneHotEncoder to team columns\n",
    "        ('numerical_for_rescaling', StandardScaler(), numerical_columns),\n",
    "    ],\n",
    ")\n",
    "pca = PCA(n_components=30)  # Reduce to 30 dimensions\n",
    "preprocessor_pca = make_pipeline(preprocessor, pca)\n",
    "\n",
    "#add early stopping and learning rate scheduler to reduce overfitting!\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "#define and compile the neural network\n",
    "model_nn = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu',input_shape=[30], kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "model_nn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "indices = []\n",
    "accuracies = []\n",
    "for train_index, test_index in tscv.split(games):\n",
    "    indices.append([train_index, test_index])\n",
    "    data_train = games.iloc[train_index]\n",
    "    target_train = wl_home[train_index]\n",
    "    data_test = games.iloc[test_index]\n",
    "    target_test = wl_home[test_index]\n",
    "    data_train_processed = preprocessor_pca.fit_transform(data_train)\n",
    "    data_test_processed = preprocessor_pca.transform(data_test)\n",
    "    #finally, train and test the neural network\n",
    "    history = model_nn.fit(data_train_processed, target_train,\n",
    "                           validation_split=0.2,  # Use a portion of training data for validation\n",
    "                           epochs=5000,            # Adjust epochs as needed\n",
    "                           batch_size=256,        # Mini-batch size\n",
    "                           callbacks = [early_stopping, lr_scheduler],\n",
    "                           verbose=1)            # Set verbose to 1 to see the training process\n",
    "    test_loss, test_accuracy = model_nn.evaluate(data_test_processed, target_test)\n",
    "    accuracies.append(test_accuracy)\n",
    "accuracies, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de02b177-868d-4576-9e9f-a1f7def60f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12817,\n",
       " [8296, 8341, 8389, 8396, 8421, 8432, 8419, 8409, 8392, 8398, 8392],\n",
       " [0.6472653507060935,\n",
       "  0.6507763127096825,\n",
       "  0.654521338846844,\n",
       "  0.6550674884918468,\n",
       "  0.657018022938285,\n",
       "  0.657876258094718,\n",
       "  0.65686198018257,\n",
       "  0.6560817664039947,\n",
       "  0.6547554029804167,\n",
       "  0.6552235312475618,\n",
       "  0.6547554029804167])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games = pd.read_csv(base_dir / \"games_almost_ready_for_training.csv\")\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "games_train = games[games['game_date'] <= datetime.datetime(2023,9,30)]\n",
    "games_test = games[games['game_date'] >= datetime.datetime(2023,10,1)]\n",
    "n_of_games_train = len(games_train)\n",
    "correct_prediction_list = []\n",
    "hca_values = list(range(0,11))\n",
    "for hca in hca_values:\n",
    "    correct_predictions = 0\n",
    "    for idx, row in games_train.iterrows():\n",
    "        if ((row['rating_home'] + hca >= row['rating_away']) and (row['wl_home'] == 'W')) or ((row['rating_home'] + hca < row['rating_away']) and (row['wl_home'] == 'L')):\n",
    "            correct_predictions += 1\n",
    "    correct_prediction_list.append(correct_predictions)\n",
    "accuracies = []\n",
    "for pred in correct_prediction_list:\n",
    "    accuracies.append(float(pred)/float(n_of_games_train))\n",
    "n_of_games_train, correct_prediction_list, accuracies\n",
    "#Conclusion : hca = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf98ffb-67d6-482c-aa51-2b12d39d1f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 900, 0.6823351023502654)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Time to test:\n",
    "#Interestingly, hca = 3 works better :) I think it's because home-court advantage is starting to matter less and less, and the change is noticeable in the last 10 years\n",
    "games_test = games[(games['game_date'] >= datetime.datetime(2023,10,1)) & ((games['game_date'] <= datetime.datetime(2024,10,1)))]\n",
    "n_of_games_test = len(games_test)\n",
    "correct_predictions = 0\n",
    "for idx, row in games_test.iterrows():\n",
    "    if ((row['rating_home'] + 3 >= row['rating_away']) and (row['wl_home'] == 'W')) or ((row['rating_home'] + 3 < row['rating_away']) and (row['wl_home'] == 'L')):\n",
    "        correct_predictions += 1\n",
    "accuracy = float(correct_predictions)/float(n_of_games_test)\n",
    "n_of_games_test, correct_predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ec1e3-c3f8-43c3-9316-0da4e3fcee30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
